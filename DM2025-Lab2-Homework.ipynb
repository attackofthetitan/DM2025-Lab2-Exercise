{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Student Information**\n",
    "Name:\n",
    "\n",
    "Student ID:\n",
    "\n",
    "GitHub ID:\n",
    "\n",
    "Kaggle name:\n",
    "\n",
    "Kaggle private scoreboard snapshot: \n",
    "\n",
    "![pic_ranking.png](./pics/pic_ranking.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Instructions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab we have divided the assignments into **three phases/parts**. The `first two phases` refer to the `exercises inside the Master notebooks` of the [DM2025-Lab2-Exercise Repo](https://github.com/difersalest/DM2025-Lab2-Exercise.git). The `third phase` refers to an `internal Kaggle competition` that we are gonna run among all the Data Mining students. Together they add up to `100 points` of your grade. There are also some `bonus points` to be gained if you complete `extra exercises` in the lab **(bonus 15 pts)** and in the `Kaggle Competition report` **(bonus 5 pts)**.\n",
    "\n",
    "**Environment recommendations to solve lab 2:**\n",
    "- **Phase 1 exercises:** Need GPU for training the models explained in that part, if you don't have a GPU in your laptop it is recommended to run in Colab or Kaggle for a faster experience, although with CPU they can still be solved but with a slower execution.\n",
    "- **Phase 2 exercises:** We use Gemini's API so everything can be run with only CPU without a problem.\n",
    "- **Phase 3 exercises:** For the competition you will probably need GPU to train your models, so it is recommended to use Colab or Kaggle if you don't have a laptop with a dedicated GPU.\n",
    "- **Optional Ollama Notebook (not graded):** You need GPU, at least 4GB of VRAM with 16 GB of RAM to run the local open-source LLM models. \n",
    "\n",
    "## **Phase 1 (30 pts):**\n",
    "\n",
    "1. __Main Exercises (25 pts):__ Do the **take home exercises** from Sections: `1. Data Preparation` to `9. High-dimension Visualization: t-SNE and UMAP`, in the [DM2025-Lab2-Master-Phase_1 Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_1.ipynb). Total: `8 exercises`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 3th, 11:59 pm, Monday)`**\n",
    "\n",
    "2. **Code Comments (5 pts):** **Tidy up the code in your notebook**. \n",
    "\n",
    "## **Phase 2 (30 pts):**\n",
    "\n",
    "1. **Main Exercises (25 pts):** Do the remaining **take home exercises** from Section: `2. Large Language Models (LLMs)` in the [DM2025-Lab2-Master-Phase_2_Main Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Main.ipynb). Total: `5 exercises required from sections 2.1, 2.2, 2.4 and 2.6`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**\n",
    "\n",
    "2. **Code Comments (5 pts):** **Tidy up the code in your notebook**. \n",
    "\n",
    "3. **`Bonus (15 pts):`** Complete the bonus exercises in the [DM2025-Lab2-Master-Phase_2_Bonus Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Bonus.ipynb) and [DM2025-Lab2-Master-Phase_2_Main Notebook](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/DM2025-Lab2-Master-Phase_2_Main.ipynb) `where 2 exercises are counted as bonus from sections 2.3 and 2.5 in the main notebook`. Total: `7 exercises`. Commit your code and submit the repository link to NTU Cool **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**\n",
    "\n",
    "## **Phase 3 (40 pts):**\n",
    "\n",
    "1. **Kaggle Competition Participation (30 pts):** Participate in the in-class **Kaggle Competition** regarding Emotion Recognition on Twitter by clicking in this link: **[Data Mining Class Kaggle Competition](https://www.kaggle.com/t/3a2df4c6d6b4417e8bf718ed648d7554)**. The scoring will be given according to your place in the Private Leaderboard ranking: \n",
    "    - **Bottom 40%**: Get 20 pts of the 30 pts in this competition participation part.\n",
    "\n",
    "    - **Top 41% - 100%**: Get (0.6N + 1 - x) / (0.6N) * 10 + 20 points, where N is the total number of participants, and x is your rank. (ie. If there are 100 participants and you rank 3rd your score will be (0.6 * 100 + 1 - 3) / (0.6 * 100) * 10 + 20 = 29.67% out of 30%.)   \n",
    "    Submit your last submission **`BEFORE the deadline (Nov. 24th, 11:59 pm, Monday)`**. Make sure to take a screenshot of your position at the end of the competition and store it as `pic_ranking.png` under the `pics` folder of this repository and rerun the cell **Student Information**.\n",
    "\n",
    "2. **Competition Report (10 pts)** A report section to be filled in inside this notebook in Markdown Format, we already provided you with the template below. You need to describe your work developing the model for the competition. The report should include a section describing briefly the following elements: \n",
    "* Your preprocessing steps.\n",
    "* The feature engineering steps.\n",
    "* Explanation of your model.\n",
    "\n",
    "* **`Bonus (5 pts):`**\n",
    "    * You will have to describe more detail in the previous steps.\n",
    "    * Mention different things you tried.\n",
    "    * Mention insights you gained. \n",
    "\n",
    "[Markdown Guide - Basic Syntax](https://www.markdownguide.org/basic-syntax/)\n",
    "\n",
    "**`Things to note for Phase 3:`**\n",
    "\n",
    "* **The code used for the competition should be in this Jupyter Notebook File** `DM2025-Lab2-Homework.ipynb`.\n",
    "\n",
    "* **Push the code used for the competition to your repository**.\n",
    "\n",
    "* **The code should have a clear separation for the same sections of the report, preprocessing, feature engineering and model explanation. Briefly comment your code for easier understanding, we provide a template at the end of this notebook.**\n",
    "\n",
    "* Showing the kaggle screenshot of the ranking plus the code in this notebook will ensure the validity of your participation and the report to obtain the corresponding points.\n",
    "\n",
    "After the competition ends you will have two days more to submit the `DM2025-Lab2-Homework.ipynb` with your report in markdown format and your code. Do everything **`BEFORE the deadline (Nov. 26th, 11:59 pm, Wednesday) to obtain 100% of the available points.`**\n",
    "\n",
    "Upload your files to your repository then submit the link to it on the corresponding NTU Cool assignment.\n",
    "\n",
    "## **Deadlines:**\n",
    "\n",
    "![lab2_deadlines](./pics/lab2_deadlines.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Next you will find the template report with some simple markdown syntax explanations, use it to structure your content.\n",
    "\n",
    "You can delete the syntax suggestions after you use them.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "# **Project Report**\n",
    "\n",
    "**Syntax:** `#` creates the largest heading (H1).\n",
    "\n",
    "---\n",
    "**Syntax:** `---` creates a horizontal rule (a separator line).\n",
    "\n",
    "## 1. Model Development (10 pts Required)\n",
    "\n",
    "**Syntax:** `##` creates a secondary heading (H2).\n",
    "\n",
    "**Describe briefly each section, you can add graphs/charts to support your explanations.**\n",
    "\n",
    "### 1.1 Preprocessing Steps\n",
    "\n",
    "**Syntax:** `###` creates a tertiary heading (H3).\n",
    "\n",
    "[Content for Preprocessing]\n",
    "\n",
    "**Example Syntax for Content:**\n",
    "*   **Bold text:** `**text**`\n",
    "*   *Italic text*: `*text*`\n",
    "*   Bullet point list:\n",
    "    * Item 1\n",
    "    * Item 2\n",
    "\n",
    "Markdown Syntax to Add Image: `![Description of the Image](./your_local_folder/name_of_the_image.png)`\n",
    "\n",
    "![Example Markdown Syntax to Add Image](./pics/example_md_img.png)\n",
    "\n",
    "### 1.2 Feature Engineering Steps\n",
    "\n",
    "[Content for Feature Engineering]\n",
    "\n",
    "### 1.3 Explanation of Your Model\n",
    "\n",
    "[Content for Model Explanation]\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Bonus Section (5 pts Optional)\n",
    "\n",
    "**Add more detail in previous sections**\n",
    "\n",
    "### 2.1 Mention Different Things You Tried\n",
    "\n",
    "[Content for Experiments]\n",
    "\n",
    "### 2.2 Mention Insights You Gained\n",
    "\n",
    "[Content for Insights]\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`From here on starts the code section for the competition.`**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Competition Code**\n",
    "\n",
    "## 1. Preprocessing Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers[torch] datasets scikit-learn pandas protobuf tiktoken sentencepiece emoji --quiet\n",
    "import os, json, re, random, string, math, html, emoji, glob\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "from typing import Optional, List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from scipy.special import softmax\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "torch.set_num_threads(4)\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding, set_seed, EarlyStoppingCallback, default_data_collator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA: True | ROCm: False | use_fp16: True | use_bf16: True\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "BASE_DIR = \"kaggle\"\n",
    "paths = {\n",
    "    \"id\": os.path.join(BASE_DIR, \"data_identification.csv\"),\n",
    "    \"emo\": os.path.join(BASE_DIR, \"emotion.csv\"),\n",
    "    \"sub\": os.path.join(BASE_DIR, \"samplesubmission.csv\"),\n",
    "    \"posts\": os.path.join(BASE_DIR, \"final_posts.json\"),\n",
    "}\n",
    "\n",
    "has_cuda = torch.cuda.is_available()\n",
    "has_rocm = getattr(torch.version, \"hip\", None) is not None\n",
    "use_fp16 = bool(has_cuda and not has_rocm)\n",
    "use_bf16 = not has_rocm and has_cuda and torch.cuda.is_bf16_supported()\n",
    "print(\"CUDA:\", has_cuda, \"| ROCm:\", has_rocm, \"| use_fp16:\", use_fp16, \"| use_bf16:\", use_bf16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "id_df  = pd.read_csv(paths[\"id\"])  \n",
    "emo_df = pd.read_csv(paths[\"emo\"])    \n",
    "sub_df = pd.read_csv(paths[\"sub\"])     \n",
    "\n",
    "with open(paths[\"posts\"], \"r\", encoding=\"utf-8\") as f:\n",
    "    posts_json = json.load(f)\n",
    "\n",
    "def extract_posts(records):\n",
    "    ids, texts, hashtags = [], [], []\n",
    "    for r in records:\n",
    "        root = r.get(\"root\", r)\n",
    "        try:\n",
    "            post = root[\"_source\"][\"post\"]\n",
    "            pid = post.get(\"post_id\") or post.get(\"id\")\n",
    "            txt = post.get(\"text\")\n",
    "            tag = post.get(\"hashtags\")\n",
    "            if pid is not None and txt is not None:\n",
    "                ids.append(pid)\n",
    "                texts.append(txt)\n",
    "                hashtags.append(tag)\n",
    "        except Exception:\n",
    "            continue\n",
    "    return pd.DataFrame({\"id\": ids, \"text\": texts, \"hashtags\": hashtags})\n",
    "\n",
    "posts_df = extract_posts(posts_json)\n",
    "df = id_df.merge(posts_df, on=\"id\", how=\"left\").merge(emo_df, on=\"id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (47890, 5) | Test: (16281, 5) | Classes: ['anger', 'disgust', 'fear', 'joy', 'sadness', 'surprise']\n"
     ]
    }
   ],
   "source": [
    "MULTI_WS    = re.compile(r\"\\s+\")\n",
    "LEAD_QUOTE  = re.compile(r\"^\\s*>+\\s*\")\n",
    "NAME_PLACE = re.compile(r\"\\[NAME\\]\", re.IGNORECASE)\n",
    "URL_PATTERN   = re.compile(r\"http\\S+|www\\.\\S+\")\n",
    "USER_PATTERN  = re.compile(r\"@\\w+\")\n",
    "EDIT_PATTERN = re.compile(r\"(edit:|tl;dr)\", re.IGNORECASE)\n",
    "\n",
    "def attach_missing_list_hashtags(text: str, hashtags) -> str:\n",
    "    words = set(w.lower() for w in re.findall(r\"\\b\\w+\\b\", text))\n",
    "    extra = []\n",
    "    for h in hashtags:\n",
    "        if not h:\n",
    "            continue\n",
    "        h_clean = str(h).strip(\"#\").strip()\n",
    "        if not h_clean:\n",
    "            continue\n",
    "        if h_clean.lower() not in words:\n",
    "            extra.append(h_clean)\n",
    "    if extra:\n",
    "        text = text + \" \" + \" \".join(extra)\n",
    "    return text\n",
    "\n",
    "    \n",
    "\n",
    "def normalize_text(text: str, hashtags: Optional[str] = None) -> str:\n",
    "    text = html.unescape(text)\n",
    "    text = text.replace(\"\\r\", \" \").replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
    "\n",
    "    # demojize\n",
    "    text = emoji.demojize(text, language=\"en\").replace(\":\", \" \")\n",
    "    \n",
    "    # normalize user mentions, urls, name placeholders\n",
    "    text = USER_PATTERN.sub(\" <USER> \", text)\n",
    "    text = URL_PATTERN.sub(\" <URL> \", text)\n",
    "    text = NAME_PLACE.sub(\" <NAME> \", text)\n",
    "\n",
    "    # remove leading > quotes\n",
    "    text = LEAD_QUOTE.sub(\"\", text)\n",
    "\n",
    "    m = EDIT_PATTERN.search(text)\n",
    "    if m:\n",
    "        text = text[:m.start()]\n",
    "        \n",
    "    if isinstance(hashtags, list) and len(hashtags) > 0:\n",
    "        tag_tokens = \" \".join(f\"#{h}\" for h in hashtags if isinstance(h, str))\n",
    "        text = f\"{text} {tag_tokens}\"\n",
    "\n",
    "    # cleanup whitespace\n",
    "    text = MULTI_WS.sub(\" \", text).strip()\n",
    "    return text\n",
    "\n",
    "train_df = df[df[\"split\"]==\"train\"].dropna(subset=[\"emotion\"]).reset_index(drop=True)\n",
    "test_df  = df[df[\"split\"]==\"test\"].reset_index(drop=True)\n",
    "\n",
    "train_df[\"text\"] = train_df.apply(lambda r: normalize_text(r[\"text\"], r[\"hashtags\"]), axis=1)\n",
    "test_df[\"text\"] = test_df.apply(lambda r: normalize_text(r[\"text\"], r[\"hashtags\"]), axis=1)\n",
    "\n",
    "labels = sorted(train_df[\"emotion\"].unique().tolist())\n",
    "label2id = {l:i for i,l in enumerate(labels)}\n",
    "id2label = {i:l for l,i in label2id.items()}\n",
    "\n",
    "y = train_df[\"emotion\"].map(label2id).values.astype(int)\n",
    "print(\"Train:\", train_df.shape, \"| Test:\", test_df.shape, \"| Classes:\", labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# helpers\n",
    "def make_ds(texts, labels=None):\n",
    "    d = {\"text\": texts}\n",
    "    if labels is not None:\n",
    "        d[\"label\"] = labels\n",
    "    return Dataset.from_dict(d)\n",
    "\n",
    "def tokenize_fn(examples, tokenizer, max_len=256):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=max_len, padding=\"max_length\",)\n",
    "\n",
    "def compute_f1(eval_pred):\n",
    "    logits = eval_pred.predictions\n",
    "    labels = eval_pred.label_ids\n",
    "\n",
    "    # logits = logits[0]\n",
    "    logits = np.asarray(logits)\n",
    "    preds = logits.argmax(axis=-1)\n",
    "\n",
    "    macro = f1_score(labels, preds, average=\"macro\")\n",
    "    micro = f1_score(labels, preds, average=\"micro\")\n",
    "    return {\"f1_macro\": macro, \"f1_micro\": micro}\n",
    "\n",
    "def augment_text(text: str) -> str:\n",
    "    words = text.split()\n",
    "    if len(words) <= 4:\n",
    "        return text\n",
    "    keep = []\n",
    "    for w in words:\n",
    "        # drop 10% of tokens at random\n",
    "        if np.random.rand() < 0.1:\n",
    "            continue\n",
    "        keep.append(w)\n",
    "    if not keep:\n",
    "        keep = words\n",
    "    return \" \".join(keep)\n",
    "    \n",
    "def get_layer(name: str, num_hidden_layers: int) -> int:\n",
    "    if \"embeddings\" in name:\n",
    "        return 0\n",
    "    for i in range(num_hidden_layers):\n",
    "        if f\"encoder.layer.{i}.\" in name:\n",
    "            return i + 1\n",
    "    return num_hidden_layers + 1\n",
    "\n",
    "def reinit_layers(model, num_layers=3):\n",
    "    if hasattr(model, \"deberta\"):\n",
    "        encoder = model.deberta.encoder\n",
    "    elif hasattr(model, \"roberta\"):\n",
    "        encoder = model.roberta.encoder\n",
    "    elif hasattr(model, \"bert\"):\n",
    "        encoder = model.bert.encoder\n",
    "    else:\n",
    "        print(\"Could not identify encoder for re-initialization.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Re-initializing the last {num_layers} layers...\")\n",
    "    for layer in encoder.layer[-num_layers:]:\n",
    "        for module in layer.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                module.weight.data.normal_(mean=0.0, std=model.config.initializer_range)\n",
    "                if module.bias is not None:\n",
    "                    module.bias.data.zero_()\n",
    "            elif isinstance(module, nn.LayerNorm):\n",
    "                module.bias.data.zero_()\n",
    "                module.weight.data.fill_(1.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Implementation Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0, weight=None, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, weight=self.weight, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        return focal_loss.sum()\n",
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "    def __init__(self,class_weight=None, rdrop_alpha: float = 0.0, layerwise_lr_decay: float = 1.0, \n",
    "                    label_smoothing_factor: float = 0.05, gamma: float = 0.0, **kwargs,):\n",
    "        super().__init__(**kwargs)\n",
    "        class_weight = torch.tensor(class_weight, dtype=torch.float)\n",
    "        self.class_weight = class_weight\n",
    "        self.gamma = float(gamma)\n",
    "        self.rdrop_alpha = float(rdrop_alpha)\n",
    "        self.layerwise_lr_decay = float(layerwise_lr_decay)\n",
    "\n",
    "    # llrd\n",
    "    def create_optimizer(self):\n",
    "        if self.optimizer is not None:\n",
    "            return self.optimizer\n",
    "\n",
    "        lr = self.args.learning_rate\n",
    "        wd = self.args.weight_decay\n",
    "        layer_decay = self.layerwise_lr_decay\n",
    "\n",
    "        if layer_decay == 1.0:\n",
    "            return super().create_optimizer()\n",
    "\n",
    "        model = self.model\n",
    "        num_layers = getattr(model.config, \"num_hidden_layers\", 12)\n",
    "\n",
    "        layer_parameters = {}\n",
    "        for name, param in model.named_parameters():\n",
    "            if not param.requires_grad:\n",
    "                continue\n",
    "            layer_id = get_layer(name, num_layers)\n",
    "            layer_parameters.setdefault(layer_id, []).append((name, param))\n",
    "\n",
    "        optimizer_grouped_parameters = []\n",
    "        max_layer_id = max(layer_parameters.keys())\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "\n",
    "        for layer_id in sorted(layer_parameters.keys()):\n",
    "            scale = layer_decay ** (max_layer_id - layer_id)\n",
    "            layer_lr = lr * scale\n",
    "            layer_params = layer_parameters[layer_id]\n",
    "\n",
    "            decay_params = [\n",
    "                p for n, p in layer_params\n",
    "                if not any(nd in n for nd in no_decay)\n",
    "            ]\n",
    "            nodecay_params = [\n",
    "                p for n, p in layer_params\n",
    "                if any(nd in n for nd in no_decay)\n",
    "            ]\n",
    "\n",
    "            if decay_params:\n",
    "                optimizer_grouped_parameters.append({\"params\": decay_params, \"lr\": layer_lr, \"weight_decay\": wd})\n",
    "            if nodecay_params:\n",
    "                optimizer_grouped_parameters.append(\n",
    "                    {\"params\": nodecay_params, \"lr\": layer_lr, \"weight_decay\": 0.0}\n",
    "                )\n",
    "\n",
    "        self.optimizer = AdamW(\n",
    "            optimizer_grouped_parameters,\n",
    "            lr=lr,\n",
    "            betas=(self.args.adam_beta1, self.args.adam_beta2),\n",
    "            eps=self.args.adam_epsilon,\n",
    "        )\n",
    "        return self.optimizer\n",
    "\n",
    "    def ce_loss(self, logits, labels):\n",
    "        if labels.ndim > 1:\n",
    "            labels = labels.view(-1)\n",
    "        labels = labels.long()\n",
    "\n",
    "        if logits.ndim > 2:\n",
    "            logits = logits.view(logits.size(0), -1)\n",
    "\n",
    "        assert logits.size(0) == labels.size(0), (f\"logits batch {logits.size()} vs labels {labels.size()}\")\n",
    "\n",
    "        weight = self.class_weight.to(logits.device)\n",
    "        gamma = self.gamma\n",
    "        if gamma > 0.0:\n",
    "            focal_loss_fn = FocalLoss(gamma=gamma, weight=weight)\n",
    "            return focal_loss_fn(logits, labels)\n",
    "        else:\n",
    "            return F.cross_entropy(logits, labels, weight=weight, label_smoothing=0.05)\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs1 = model(**inputs)\n",
    "        logits1 = outputs1.logits\n",
    "\n",
    "        if self.rdrop_alpha > 0.0:\n",
    "            outputs2 = model(**inputs)\n",
    "            logits2 = outputs2.logits\n",
    "\n",
    "            loss1 = self.ce_loss(logits1, labels)\n",
    "            loss2 = self.ce_loss(logits2, labels)\n",
    "            loss_ce = 0.5 * (loss1 + loss2)\n",
    "\n",
    "            # symmetric kl on softmax\n",
    "            p_log = F.log_softmax(logits1, dim=-1)\n",
    "            q_log = F.log_softmax(logits2, dim=-1)\n",
    "            p = p_log.exp()\n",
    "            q = q_log.exp()\n",
    "            kl_pq = F.kl_div(p_log, q, reduction=\"batchmean\")\n",
    "            kl_qp = F.kl_div(q_log, p, reduction=\"batchmean\")\n",
    "            loss_kl = 0.5 * (kl_pq + kl_qp)\n",
    "\n",
    "            loss = loss_ce + self.rdrop_alpha * loss_kl\n",
    "\n",
    "            if return_outputs:\n",
    "                avg_logits = 0.5 * (logits1 + logits2)\n",
    "                outputs1.logits = avg_logits\n",
    "                return loss, outputs1\n",
    "            return loss\n",
    "        else:\n",
    "            loss = self.ce_loss(logits1, labels)\n",
    "            if return_outputs:\n",
    "                return loss, outputs1\n",
    "            return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "class MultiHeadClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_labels, n_heads=4, p=0.2):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.dropouts = nn.ModuleList([nn.Dropout(p) for _ in range(n_heads)])\n",
    "        self.classifiers = nn.ModuleList(\n",
    "            [nn.Linear(input_dim, num_labels) for _ in range(n_heads)]\n",
    "        )\n",
    "\n",
    "        for classifier in self.classifiers:\n",
    "            nn.init.normal_(classifier.weight, mean=0.0, std=0.02)\n",
    "            nn.init.zeros_(classifier.bias)        \n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 3:\n",
    "            x = x[:, 0, :]  # [B, H]\n",
    "\n",
    "        logits = 0\n",
    "        for i in range(self.n_heads):\n",
    "            logits = logits + self.classifiers[i](self.dropouts[i](x))\n",
    "        return logits / self.n_heads  # [B, num_labels]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "def train_folds(model_name, n_splits=5, lr=2e-5, epochs=3, bsz=16, max_len=192, wd=0.01, seed=SEED, n_heads=4, head_dropout=None):\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "    new_tokens = [\"<USER>\", \"<URL>\", \"<NAME>\"]\n",
    "    new_tokens = [t for t in new_tokens if t not in tokenizer.get_vocab()]\n",
    "    \n",
    "    if new_tokens:\n",
    "        print(f\"Adding {len(new_tokens)} special tokens: {new_tokens}\")\n",
    "        tokenizer.add_tokens(new_tokens)\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "\n",
    "    oof_preds = np.zeros((len(train_df), len(labels)), dtype=np.float32)\n",
    "    test_logits_accum = np.zeros((len(test_df), len(labels)), dtype=np.float32)\n",
    "\n",
    "    for fold, (tr_idx, va_idx) in enumerate(skf.split(train_df, y), 1):\n",
    "        output_dir = f\"./out_{os.path.basename(model_name)}_f{fold}\"\n",
    "        print(f\"\\n[{model_name}] Fold {fold}/{n_splits}\")\n",
    "        tr_df = train_df.iloc[tr_idx].copy()\n",
    "\n",
    "        tr_texts = tr_df[\"text\"].tolist()\n",
    "        tr_labels = tr_df[\"emotion\"].map(label2id).tolist()\n",
    "\n",
    "        va_texts = train_df.loc[va_idx, \"text\"].tolist()\n",
    "        va_labels = train_df.loc[va_idx, \"emotion\"].map(label2id).tolist()\n",
    "\n",
    "        ds_tr = make_ds(tr_texts, tr_labels).map(lambda ex: tokenize_fn(ex, tokenizer, max_len), batched=True)\n",
    "        ds_va = make_ds(va_texts, va_labels).map(lambda ex: tokenize_fn(ex, tokenizer, max_len), batched=True)\n",
    "\n",
    "        tr_y = y[tr_idx]\n",
    "        classes = np.unique(tr_y)\n",
    "        raw_class_weight = compute_class_weight(\"balanced\", classes=classes, y=tr_y)\n",
    "        gamma = 0.5\n",
    "        class_weight = np.power(raw_class_weight, gamma)\n",
    "\n",
    "        ds_te = make_ds(test_df[\"text\"].tolist()).map(lambda ex: tokenize_fn(ex, tokenizer, max_len), batched=True)\n",
    "\n",
    "        cols = [\"input_ids\",\"attention_mask\",\"label\"]\n",
    "        ds_tr.set_format(type=\"torch\", columns=[c for c in cols if c in ds_tr.column_names])\n",
    "        ds_va.set_format(type=\"torch\", columns=[c for c in cols if c in ds_va.column_names])\n",
    "        ds_te.set_format(type=\"torch\", columns=[c for c in [\"input_ids\",\"attention_mask\"] if c in ds_te.column_names])\n",
    "\n",
    "        config = AutoConfig.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=len(labels),\n",
    "            label2id=label2id,\n",
    "            id2label=id2label,\n",
    "            hidden_dropout_prob=0.2,\n",
    "            attention_probs_dropout_prob=0.2,\n",
    "        )\n",
    "\n",
    "        config.problem_type = \"single_label_classification\"\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            config=config,\n",
    "            ignore_mismatched_sizes=True,\n",
    "        )\n",
    "        \n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "        # reinit_layers(model, num_layers=3)\n",
    "        hidden_size = config.hidden_size\n",
    "        \n",
    "        if head_dropout is None:\n",
    "            head_dropout = config.hidden_dropout_prob\n",
    "            \n",
    "        model.classifier = MultiHeadClassifier(\n",
    "            input_dim=hidden_size,\n",
    "            num_labels=len(labels),\n",
    "            n_heads=n_heads,\n",
    "            p=head_dropout,\n",
    "        )\n",
    "\n",
    "        args = TrainingArguments(\n",
    "            output_dir=f\"./out_{os.path.basename(model_name)}_f{fold}\",\n",
    "            learning_rate=lr,\n",
    "            per_device_train_batch_size=bsz,\n",
    "            per_device_eval_batch_size=bsz*2,\n",
    "            num_train_epochs=epochs,\n",
    "            weight_decay=wd,\n",
    "            eval_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"f1_macro\", \n",
    "            save_total_limit=1,\n",
    "            logging_steps=50,\n",
    "            greater_is_better=True,\n",
    "            report_to=\"none\",\n",
    "            seed=seed+fold,\n",
    "            fp16=use_fp16 and not use_bf16,\n",
    "            bf16=use_bf16,\n",
    "            lr_scheduler_type=\"cosine\",\n",
    "            warmup_ratio=0.1,\n",
    "            max_grad_norm=0.5,\n",
    "            optim=\"adamw_torch\",\n",
    "        )\n",
    "\n",
    "        data_collator = default_data_collator \n",
    "        trainer = WeightedTrainer(\n",
    "            class_weight=class_weight,\n",
    "            rdrop_alpha=0.5,\n",
    "            layerwise_lr_decay=0.95,\n",
    "            label_smoothing_factor=0.05,\n",
    "            model=model,\n",
    "            gamma=0.0,\n",
    "            args=args,\n",
    "            train_dataset=ds_tr,\n",
    "            eval_dataset=ds_va,\n",
    "            processing_class=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "            compute_metrics=compute_f1,\n",
    "            callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    "        )\n",
    "\n",
    "        resume_ckpt = None\n",
    "        if os.path.isdir(output_dir):\n",
    "            ckpts = glob.glob(os.path.join(output_dir, \"checkpoint-*\"))\n",
    "            if ckpts:\n",
    "                ckpts_sorted = sorted(ckpts, key=lambda p: int(p.split(\"-\")[-1]))\n",
    "                resume_ckpt = ckpts_sorted[-1]\n",
    "\n",
    "        if resume_ckpt is not None:\n",
    "            print(\"Resuming from checkpoint:\", resume_ckpt)\n",
    "            trainer.train(resume_from_checkpoint=resume_ckpt)\n",
    "        else:\n",
    "            print(\"No checkpoint found, training from scratch.\")\n",
    "            trainer.train()\n",
    "\n",
    "        valid_logits = trainer.predict(ds_va).predictions\n",
    "        oof_preds[va_idx] = valid_logits\n",
    "\n",
    "        test_logits = trainer.predict(ds_te).predictions\n",
    "        test_logits_accum += test_logits / n_splits\n",
    "\n",
    "        va_pred = np.argmax(valid_logits, axis=1)\n",
    "        f1 = f1_score(va_labels, va_pred, average=\"macro\")\n",
    "        print(f\"Fold {fold} F1(macro): {f1:.4f}\")\n",
    "\n",
    "        del trainer, model\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    oof_pred_labels = oof_preds.argmax(axis=1)\n",
    "    oof_f1_macro = f1_score(y, oof_pred_labels, average=\"macro\")\n",
    "    oof_f1_micro = f1_score(y, oof_pred_labels, average=\"micro\")\n",
    "    print(f\"\\n[Model {model_name}] OOF F1(macro): {oof_f1_macro:.4f} | micro: {oof_f1_micro:.4f}\")\n",
    "\n",
    "    return oof_preds, test_logits_accum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Amirhossein75/multi-label-emotion-classification-reddit-comments-roberta | SEED 42 ====\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ab701b1fc5e4156b7406dca9b6fb361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ae1d849d81d46cb805f19c1273f2f04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5c40d6b0a214776a80b61f147504759",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "787ef7d27a224430bc940163abc16bb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9829c5438004ad39276c7ef894a9a0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 3 special tokens: ['<USER>', '<URL>', '<NAME>']\n",
      "\n",
      "[Amirhossein75/multi-label-emotion-classification-reddit-comments-roberta] Fold 1/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01b2b35db8ee4439a10d914717de22cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/38312 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b734924ca824247a328998bffd686e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9578 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "256b80d68c26452da24fcbd17b820394",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16281 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4176929812ea458db0ae5ea716dc2d21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21ea0fe9a5c44602b3d57ea4ff86cfaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at Amirhossein75/multi-label-emotion-classification-reddit-comments-roberta and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.bias: found shape torch.Size([28]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "- classifier.out_proj.weight: found shape torch.Size([28, 768]) in the checkpoint and torch.Size([6, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found, training from scratch.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9580' max='9580' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9580/9580 13:08, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.270600</td>\n",
       "      <td>1.223736</td>\n",
       "      <td>0.540830</td>\n",
       "      <td>0.680100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.192700</td>\n",
       "      <td>1.200426</td>\n",
       "      <td>0.564657</td>\n",
       "      <td>0.698267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.092300</td>\n",
       "      <td>1.206045</td>\n",
       "      <td>0.565504</td>\n",
       "      <td>0.697327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.081500</td>\n",
       "      <td>1.206154</td>\n",
       "      <td>0.567594</td>\n",
       "      <td>0.699102</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 F1(macro): 0.5676\n",
      "\n",
      "[Amirhossein75/multi-label-emotion-classification-reddit-comments-roberta] Fold 2/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc29375d3bb045a98117a9c252c77835",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/38312 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "867230786ed14cd28bcc39f75685c359",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9578 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f66019503424400a7c958d89f09d3e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16281 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at Amirhossein75/multi-label-emotion-classification-reddit-comments-roberta and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.bias: found shape torch.Size([28]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "- classifier.out_proj.weight: found shape torch.Size([28, 768]) in the checkpoint and torch.Size([6, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found, training from scratch.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9580' max='9580' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9580/9580 14:01, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.242300</td>\n",
       "      <td>1.188834</td>\n",
       "      <td>0.561820</td>\n",
       "      <td>0.691585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.241600</td>\n",
       "      <td>1.170189</td>\n",
       "      <td>0.572573</td>\n",
       "      <td>0.691167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.121300</td>\n",
       "      <td>1.177490</td>\n",
       "      <td>0.576594</td>\n",
       "      <td>0.702861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.158800</td>\n",
       "      <td>1.180972</td>\n",
       "      <td>0.572622</td>\n",
       "      <td>0.699729</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 F1(macro): 0.5766\n",
      "\n",
      "[Amirhossein75/multi-label-emotion-classification-reddit-comments-roberta] Fold 3/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "068ff855a790439cb12008bac24175f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/38312 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bccf25070c546d29b63c96fa3437e8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9578 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b4066a7deb94d7b87b7ddb9787ec03e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16281 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at Amirhossein75/multi-label-emotion-classification-reddit-comments-roberta and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.bias: found shape torch.Size([28]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "- classifier.out_proj.weight: found shape torch.Size([28, 768]) in the checkpoint and torch.Size([6, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found, training from scratch.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9580' max='9580' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9580/9580 13:55, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.352400</td>\n",
       "      <td>1.225993</td>\n",
       "      <td>0.557034</td>\n",
       "      <td>0.689706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.187600</td>\n",
       "      <td>1.206245</td>\n",
       "      <td>0.558533</td>\n",
       "      <td>0.687095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.112300</td>\n",
       "      <td>1.214068</td>\n",
       "      <td>0.562983</td>\n",
       "      <td>0.693882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.113400</td>\n",
       "      <td>1.216588</td>\n",
       "      <td>0.565192</td>\n",
       "      <td>0.692420</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 F1(macro): 0.5652\n",
      "\n",
      "[Amirhossein75/multi-label-emotion-classification-reddit-comments-roberta] Fold 4/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "861df98b8f3d425882cec0904a9d0286",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/38312 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29ebf353c28a45daad6ea01c40f096f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9578 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "928ec998666a4588bc8afd85a31b6da5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16281 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at Amirhossein75/multi-label-emotion-classification-reddit-comments-roberta and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.bias: found shape torch.Size([28]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "- classifier.out_proj.weight: found shape torch.Size([28, 768]) in the checkpoint and torch.Size([6, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found, training from scratch.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9580' max='9580' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9580/9580 13:59, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.245500</td>\n",
       "      <td>1.207431</td>\n",
       "      <td>0.554199</td>\n",
       "      <td>0.671852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.232000</td>\n",
       "      <td>1.204339</td>\n",
       "      <td>0.559169</td>\n",
       "      <td>0.687409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.159100</td>\n",
       "      <td>1.195959</td>\n",
       "      <td>0.564194</td>\n",
       "      <td>0.693569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.031100</td>\n",
       "      <td>1.203804</td>\n",
       "      <td>0.565357</td>\n",
       "      <td>0.693569</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 F1(macro): 0.5654\n",
      "\n",
      "[Amirhossein75/multi-label-emotion-classification-reddit-comments-roberta] Fold 5/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f9af2614bf24e2aa22b9a63f08a1193",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/38312 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c58e315ff684d81bc8afb496bbec5ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9578 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f1ff40cca234cf1a98df9c28c0be71e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16281 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at Amirhossein75/multi-label-emotion-classification-reddit-comments-roberta and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.bias: found shape torch.Size([28]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "- classifier.out_proj.weight: found shape torch.Size([28, 768]) in the checkpoint and torch.Size([6, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found, training from scratch.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9580' max='9580' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9580/9580 13:45, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.265300</td>\n",
       "      <td>1.232774</td>\n",
       "      <td>0.532210</td>\n",
       "      <td>0.673940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.203300</td>\n",
       "      <td>1.223185</td>\n",
       "      <td>0.545986</td>\n",
       "      <td>0.686573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.163000</td>\n",
       "      <td>1.226115</td>\n",
       "      <td>0.549050</td>\n",
       "      <td>0.686051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.137800</td>\n",
       "      <td>1.229738</td>\n",
       "      <td>0.552680</td>\n",
       "      <td>0.688557</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 F1(macro): 0.5527\n",
      "\n",
      "[Model Amirhossein75/multi-label-emotion-classification-reddit-comments-roberta] OOF F1(macro): 0.5657 | micro: 0.6953\n",
      "\n",
      "==== Amirhossein75/multi-label-emotion-classification-reddit-comments-roberta | SEED 2025 ====\n",
      "Adding 3 special tokens: ['<USER>', '<URL>', '<NAME>']\n",
      "\n",
      "[Amirhossein75/multi-label-emotion-classification-reddit-comments-roberta] Fold 1/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c85765d29564ac78d0b0929820b47a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/38312 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c23f1e54b6c2491b906dcbc92a989791",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9578 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "103f05237d454acb8ecb709eea2f9697",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16281 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at Amirhossein75/multi-label-emotion-classification-reddit-comments-roberta and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.bias: found shape torch.Size([28]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "- classifier.out_proj.weight: found shape torch.Size([28, 768]) in the checkpoint and torch.Size([6, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from checkpoint: ./out_multi-label-emotion-classification-reddit-comments-roberta_f1/checkpoint-9580\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9580' max='9580' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9580/9580 : < :, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 F1(macro): 0.6280\n",
      "\n",
      "[Amirhossein75/multi-label-emotion-classification-reddit-comments-roberta] Fold 2/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "631a271330504737b581408fb44f262c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/38312 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b3c5f7b8ba84281801abc22ec5dac5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9578 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab2e0238206440b89506fdb8d1bbaa74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16281 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at Amirhossein75/multi-label-emotion-classification-reddit-comments-roberta and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.bias: found shape torch.Size([28]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "- classifier.out_proj.weight: found shape torch.Size([28, 768]) in the checkpoint and torch.Size([6, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from checkpoint: ./out_multi-label-emotion-classification-reddit-comments-roberta_f2/checkpoint-7185\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9580' max='9580' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9580/9580 03:13, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.162600</td>\n",
       "      <td>1.076497</td>\n",
       "      <td>0.608350</td>\n",
       "      <td>0.731886</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 F1(macro): 0.6084\n",
      "\n",
      "[Amirhossein75/multi-label-emotion-classification-reddit-comments-roberta] Fold 3/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9d9ad77b1f149a2a51114929f1a109a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/38312 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9afbf84c0fb4f2cb3b567a906078477",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9578 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e8ee69c1c144396a990962def3ba300",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16281 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at Amirhossein75/multi-label-emotion-classification-reddit-comments-roberta and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.bias: found shape torch.Size([28]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "- classifier.out_proj.weight: found shape torch.Size([28, 768]) in the checkpoint and torch.Size([6, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from checkpoint: ./out_multi-label-emotion-classification-reddit-comments-roberta_f3/checkpoint-9580\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9580' max='9580' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9580/9580 : < :, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 F1(macro): 0.6331\n",
      "\n",
      "[Amirhossein75/multi-label-emotion-classification-reddit-comments-roberta] Fold 4/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "360bd3dc5be643fd9f06fa59e4c1f734",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/38312 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cfe877eb3004d54970feb00b1c211f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9578 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da665a3dc0754056b684e3190e0be03c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16281 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at Amirhossein75/multi-label-emotion-classification-reddit-comments-roberta and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.bias: found shape torch.Size([28]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "- classifier.out_proj.weight: found shape torch.Size([28, 768]) in the checkpoint and torch.Size([6, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from checkpoint: ./out_multi-label-emotion-classification-reddit-comments-roberta_f4/checkpoint-9580\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9580' max='9580' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9580/9580 : < :, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 F1(macro): 0.6373\n",
      "\n",
      "[Amirhossein75/multi-label-emotion-classification-reddit-comments-roberta] Fold 5/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8af17e74c0084488971ea197dda16ae8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/38312 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f33c46a513c84dea9986a24288748df9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9578 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "407cb7b2114b48ffaa5c15d75ca0c353",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16281 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at Amirhossein75/multi-label-emotion-classification-reddit-comments-roberta and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.bias: found shape torch.Size([28]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "- classifier.out_proj.weight: found shape torch.Size([28, 768]) in the checkpoint and torch.Size([6, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from checkpoint: ./out_multi-label-emotion-classification-reddit-comments-roberta_f5/checkpoint-9580\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9580' max='9580' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9580/9580 : < :, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 F1(macro): 0.6371\n",
      "\n",
      "[Model Amirhossein75/multi-label-emotion-classification-reddit-comments-roberta] OOF F1(macro): 0.6287 | micro: 0.7389\n",
      "\n",
      "==== cardiffnlp/twitter-roberta-large-emotion-latest | SEED 42 ====\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e66933256e4e4c0bafeab41779d79be2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b64b72a377e7410ca66c2006d3b8224f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9dbd687519b4903bb67f3c7427dadfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2c1b29409344a22921c3ba787fc2e3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c9470e42f52447a93eee17b301dbcfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/280 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 3 special tokens: ['<USER>', '<URL>', '<NAME>']\n",
      "\n",
      "[cardiffnlp/twitter-roberta-large-emotion-latest] Fold 1/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d32e8234141480e80a06110d0c53ea8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/38312 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1b27cb46c8b4804800f210529878cc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9578 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6332bad853b1441d87bc1e27298842b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16281 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8577961dd7b4ad4ab9dceb1c337b044",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "479fbc67538f483ebaae74dd644bcdb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-large-emotion-latest and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.bias: found shape torch.Size([11]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "- classifier.out_proj.weight: found shape torch.Size([11, 1024]) in the checkpoint and torch.Size([6, 1024]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found, training from scratch.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9580' max='9580' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9580/9580 30:51, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.248100</td>\n",
       "      <td>1.236786</td>\n",
       "      <td>0.531281</td>\n",
       "      <td>0.680309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.189500</td>\n",
       "      <td>1.204612</td>\n",
       "      <td>0.552538</td>\n",
       "      <td>0.693986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.036000</td>\n",
       "      <td>1.230112</td>\n",
       "      <td>0.558247</td>\n",
       "      <td>0.688975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.992900</td>\n",
       "      <td>1.239312</td>\n",
       "      <td>0.558536</td>\n",
       "      <td>0.690541</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 F1(macro): 0.5585\n",
      "\n",
      "[cardiffnlp/twitter-roberta-large-emotion-latest] Fold 2/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d4aa85ae6b841a08c547e75c67133eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/38312 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97f6db6f34de4a5281b10da84b6eb733",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9578 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfcfc9ed342849ec9a056a4444af99a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16281 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-large-emotion-latest and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.bias: found shape torch.Size([11]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "- classifier.out_proj.weight: found shape torch.Size([11, 1024]) in the checkpoint and torch.Size([6, 1024]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found, training from scratch.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9580' max='9580' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9580/9580 31:00, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.238600</td>\n",
       "      <td>1.205478</td>\n",
       "      <td>0.551929</td>\n",
       "      <td>0.686782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.175376</td>\n",
       "      <td>0.571314</td>\n",
       "      <td>0.686678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.049700</td>\n",
       "      <td>1.203597</td>\n",
       "      <td>0.577321</td>\n",
       "      <td>0.701086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.031700</td>\n",
       "      <td>1.211092</td>\n",
       "      <td>0.578663</td>\n",
       "      <td>0.701086</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 F1(macro): 0.5787\n",
      "\n",
      "[cardiffnlp/twitter-roberta-large-emotion-latest] Fold 3/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df5db88776774a059c241827bf3ce996",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/38312 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "801c60c8f16f49a788c86fe38a2475a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9578 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31fedee98da64d9b93516aa016d34299",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16281 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-large-emotion-latest and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.bias: found shape torch.Size([11]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "- classifier.out_proj.weight: found shape torch.Size([11, 1024]) in the checkpoint and torch.Size([6, 1024]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found, training from scratch.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9580' max='9580' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9580/9580 30:53, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.303900</td>\n",
       "      <td>1.246853</td>\n",
       "      <td>0.537917</td>\n",
       "      <td>0.684903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.156900</td>\n",
       "      <td>1.206464</td>\n",
       "      <td>0.554096</td>\n",
       "      <td>0.683754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.043900</td>\n",
       "      <td>1.227543</td>\n",
       "      <td>0.552263</td>\n",
       "      <td>0.689392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.045800</td>\n",
       "      <td>1.245221</td>\n",
       "      <td>0.557507</td>\n",
       "      <td>0.688035</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 F1(macro): 0.5575\n",
      "\n",
      "[cardiffnlp/twitter-roberta-large-emotion-latest] Fold 4/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc684037e20248b8bc6b115cf83cf35a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/38312 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfece61ecc02400faf3e4eb94f3281fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9578 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecd8397fe9804efd8f94f79be656531a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16281 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-large-emotion-latest and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.bias: found shape torch.Size([11]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "- classifier.out_proj.weight: found shape torch.Size([11, 1024]) in the checkpoint and torch.Size([6, 1024]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found, training from scratch.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9580' max='9580' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9580/9580 30:53, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.250200</td>\n",
       "      <td>1.220811</td>\n",
       "      <td>0.551393</td>\n",
       "      <td>0.671121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.222100</td>\n",
       "      <td>1.221369</td>\n",
       "      <td>0.557658</td>\n",
       "      <td>0.681040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.098300</td>\n",
       "      <td>1.216341</td>\n",
       "      <td>0.557443</td>\n",
       "      <td>0.686469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.943600</td>\n",
       "      <td>1.238937</td>\n",
       "      <td>0.563875</td>\n",
       "      <td>0.690019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 F1(macro): 0.5639\n",
      "\n",
      "[cardiffnlp/twitter-roberta-large-emotion-latest] Fold 5/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80ae6b94a7044c6db0b6ae06735ae24a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/38312 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6c90fe9035340be92bb72440bfc2cb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9578 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4238c89136544feb85b3ced5db3a76e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16281 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-large-emotion-latest and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.bias: found shape torch.Size([11]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "- classifier.out_proj.weight: found shape torch.Size([11, 1024]) in the checkpoint and torch.Size([6, 1024]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoint found, training from scratch.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9580' max='9580' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9580/9580 30:51, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.275800</td>\n",
       "      <td>1.245222</td>\n",
       "      <td>0.517021</td>\n",
       "      <td>0.670182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.167400</td>\n",
       "      <td>1.226560</td>\n",
       "      <td>0.548456</td>\n",
       "      <td>0.683232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.102700</td>\n",
       "      <td>1.243604</td>\n",
       "      <td>0.550810</td>\n",
       "      <td>0.686156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.022600</td>\n",
       "      <td>1.259688</td>\n",
       "      <td>0.552918</td>\n",
       "      <td>0.685843</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 F1(macro): 0.5529\n",
      "\n",
      "[Model cardiffnlp/twitter-roberta-large-emotion-latest] OOF F1(macro): 0.5624 | micro: 0.6911\n",
      "\n",
      "==== cardiffnlp/twitter-roberta-large-emotion-latest | SEED 2025 ====\n",
      "Adding 3 special tokens: ['<USER>', '<URL>', '<NAME>']\n",
      "\n",
      "[cardiffnlp/twitter-roberta-large-emotion-latest] Fold 1/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e3751b844534b4eb062595e5aaf6998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/38312 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b1f8a68df9c4e3d81c8a8fe17f9e567",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9578 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e044d80c60c9433b8ded8235e399f6f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16281 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-large-emotion-latest and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.bias: found shape torch.Size([11]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "- classifier.out_proj.weight: found shape torch.Size([11, 1024]) in the checkpoint and torch.Size([6, 1024]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from checkpoint: ./out_twitter-roberta-large-emotion-latest_f1/checkpoint-9580\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9580' max='9580' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9580/9580 : < :, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 F1(macro): 0.6640\n",
      "\n",
      "[cardiffnlp/twitter-roberta-large-emotion-latest] Fold 2/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b921a1238b11407585645bd7c09d346d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/38312 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba44f4856b6c410481e9cd986ade103c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9578 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e385c80d61c40138c247a6afcda905e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16281 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-large-emotion-latest and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.bias: found shape torch.Size([11]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "- classifier.out_proj.weight: found shape torch.Size([11, 1024]) in the checkpoint and torch.Size([6, 1024]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from checkpoint: ./out_twitter-roberta-large-emotion-latest_f2/checkpoint-9580\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9580' max='9580' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9580/9580 : < :, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 F1(macro): 0.6570\n",
      "\n",
      "[cardiffnlp/twitter-roberta-large-emotion-latest] Fold 3/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd8ac3566c424cf9914fa569de67de4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/38312 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3662015b85e4ea59e970e07d94b6339",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9578 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e2371b9d2404502926beab14e453ecd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16281 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-large-emotion-latest and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.bias: found shape torch.Size([11]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "- classifier.out_proj.weight: found shape torch.Size([11, 1024]) in the checkpoint and torch.Size([6, 1024]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from checkpoint: ./out_twitter-roberta-large-emotion-latest_f3/checkpoint-9580\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9580' max='9580' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9580/9580 : < :, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 F1(macro): 0.6763\n",
      "\n",
      "[cardiffnlp/twitter-roberta-large-emotion-latest] Fold 4/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c0075c0bcce401abd2f77192f8ad390",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/38312 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e205652bce944ab7958a25cdf21a414e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9578 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "955d5d2eef2e4b519d2ca912dbeaa5e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16281 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-large-emotion-latest and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.bias: found shape torch.Size([11]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "- classifier.out_proj.weight: found shape torch.Size([11, 1024]) in the checkpoint and torch.Size([6, 1024]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from checkpoint: ./out_twitter-roberta-large-emotion-latest_f4/checkpoint-9580\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9580' max='9580' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9580/9580 : < :, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 F1(macro): 0.6782\n",
      "\n",
      "[cardiffnlp/twitter-roberta-large-emotion-latest] Fold 5/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ad509a3c80d4b00bb3c29609659e561",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/38312 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddf1cea63cbc4455ba9e70559e364e42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9578 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c74111effc7b4a3199a1bb0427fd8112",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16281 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-large-emotion-latest and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.bias: found shape torch.Size([11]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "- classifier.out_proj.weight: found shape torch.Size([11, 1024]) in the checkpoint and torch.Size([6, 1024]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from checkpoint: ./out_twitter-roberta-large-emotion-latest_f5/checkpoint-9580\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9580' max='9580' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9580/9580 : < :, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 F1(macro): 0.6801\n",
      "\n",
      "[Model cardiffnlp/twitter-roberta-large-emotion-latest] OOF F1(macro): 0.6712 | micro: 0.7649\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "SEEDS = [42, 2025] \n",
    "\n",
    "backbones = [\n",
    "    # \"GroNLP/hateBERT\",\n",
    "    # \"microsoft/deberta-v3-large\",\n",
    "    # \"FacebookAI/roberta-large\",\n",
    "    # \"j-hartmann/emotion-english-roberta-large\",\n",
    "    \"Amirhossein75/multi-label-emotion-classification-reddit-comments-roberta\",\n",
    "    \"cardiffnlp/twitter-roberta-large-emotion-latest\",\n",
    "]\n",
    "cfgs = {\n",
    "    \"j-hartmann/emotion-english-roberta-large\": dict(lr=1.5e-5, epochs=4),\n",
    "    \"FacebookAI/roberta-large\": dict(lr=1.5e-5, epochs=4),\n",
    "    \"microsoft/deberta-v3-large\": dict(lr=8e-6, epochs=4),\n",
    "    \"Amirhossein75/multi-label-emotion-classification-reddit-comments-roberta\": dict(lr=1.5e-5, epochs=4),\n",
    "    \"GroNLP/hateBERT\": dict(lr=1.5e-5, epochs=4),\n",
    "    \"cardiffnlp/twitter-roberta-large-emotion-latest\": dict(lr=1.5e-5, epochs=4),\n",
    "}\n",
    "\n",
    "all_oof, all_test = [], []\n",
    "\n",
    "for name in backbones:\n",
    "    hp = cfgs[name]\n",
    "    for s in SEEDS:\n",
    "        print(f\"\\n==== {name} | SEED {s} ====\")\n",
    "        oof, test_logits = train_folds(\n",
    "            name,\n",
    "            n_splits=5,\n",
    "            lr=hp[\"lr\"],\n",
    "            epochs=hp[\"epochs\"],\n",
    "            bsz=16,\n",
    "            max_len=128,\n",
    "            wd=0.01,\n",
    "            seed=s,\n",
    "        )\n",
    "        all_oof.append(oof)\n",
    "        all_test.append(test_logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.8. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Stacked Ensemble] OOF F1(macro): 0.7726\n",
      "[Simple Average] OOF F1(macro): 0.6194\n",
      "Using stacked ensemble\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.69      0.68      0.69     10694\n",
      "     disgust       0.35      0.34      0.35      1183\n",
      "        fear       0.55      0.76      0.64      2009\n",
      "         joy       0.87      0.82      0.84     23797\n",
      "     sadness       0.53      0.59      0.56      3926\n",
      "    surprise       0.62      0.66      0.64      6281\n",
      "\n",
      "    accuracy                           0.73     47890\n",
      "   macro avg       0.60      0.64      0.62     47890\n",
      "weighted avg       0.74      0.73      0.74     47890\n",
      "\n",
      "[[ 7278   372   347  1221   748   728]\n",
      " [  364   406   109   108   144    52]\n",
      " [  106    62  1528   121   118    74]\n",
      " [ 1563   161   444 19461   723  1445]\n",
      " [  629   105   194   464  2298   236]\n",
      " [  586    48   139  1046   286  4176]]\n"
     ]
    }
   ],
   "source": [
    "def optimized_ensemble(all_oof, all_test, y):\n",
    "    X_oof = np.hstack(all_oof)\n",
    "    \n",
    "    meta_model = LogisticRegression(\n",
    "        multi_class='multinomial',\n",
    "        max_iter=1000,\n",
    "        C=0.1,\n",
    "        random_state=SEED\n",
    "    )\n",
    "    meta_model.fit(X_oof, y)\n",
    "    \n",
    "    oof_pred = meta_model.predict(X_oof)\n",
    "    oof_f1 = f1_score(y, oof_pred, average=\"macro\")\n",
    "    print(f\"\\n[Stacked Ensemble] OOF F1(macro): {oof_f1:.4f}\")\n",
    "    \n",
    "    X_test = np.hstack(all_test)\n",
    "    test_proba = meta_model.predict_proba(X_test)\n",
    "    \n",
    "    return test_proba, oof_pred\n",
    "\n",
    "test_proba, oof_pred = optimized_ensemble(all_oof, all_test, y)\n",
    "\n",
    "oof_blend = np.mean(all_oof, axis=0)\n",
    "test_blend = np.mean(all_test, axis=0)\n",
    "\n",
    "oof_pred_avg = oof_blend.argmax(axis=1)\n",
    "oof_f1_avg = f1_score(y, oof_pred_avg, average=\"macro\")\n",
    "print(f\"[Simple Average] OOF F1(macro): {oof_f1_avg:.4f}\")\n",
    "\n",
    "if f1_score(y, oof_pred, average=\"macro\") > oof_f1_avg:\n",
    "    print(\"Using stacked ensemble\")\n",
    "    final_predictions = test_proba.argmax(axis=1)\n",
    "else:\n",
    "    print(\"Using simple average\")\n",
    "    final_predictions = test_blend.argmax(axis=1)\n",
    "\n",
    "oof_pred = oof_blend.argmax(axis=1)\n",
    "print(classification_report(y, oof_pred, target_names=labels))\n",
    "\n",
    "cm = confusion_matrix(y, oof_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ./submit/submission.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0x61fc95</td>\n",
       "      <td>fear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0xaba820</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0x66e44d</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0xc03cf5</td>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0x02f65a</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id emotion\n",
       "0  0x61fc95    fear\n",
       "1  0xaba820     joy\n",
       "2  0x66e44d     joy\n",
       "3  0xc03cf5     joy\n",
       "4  0x02f65a   anger"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels = [id2label[i] for i in final_predictions]\n",
    "submission = pd.DataFrame({\"id\": test_df[\"id\"].values, \"emotion\": test_labels})\n",
    "sub_path = \"./submit/submission.csv\"\n",
    "os.makedirs(\"./submit\", exist_ok=True)\n",
    "submission.to_csv(sub_path, index=False)\n",
    "print(\"Saved:\", sub_path)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Train Size: 47890\n",
      "Adding 4344 pseudo-labels (Confidence > 0.95)\n",
      "New Train Size: 52234\n",
      "\n",
      "[Pseudo-Label Retrain] Model: Amirhossein75/multi-label-emotion-classification-reddit-comments-roberta\n",
      "Adding 3 special tokens: ['<USER>', '<URL>', '<NAME>']\n",
      "\n",
      "[Amirhossein75/multi-label-emotion-classification-reddit-comments-roberta] Fold 1/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f378b4c6af6f4f1db3a305583dc866e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/41787 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5209b79892494b5892805c4b0e0c8001",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10447 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0958bd6b0524da7a4babb7b151fe0c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16281 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at Amirhossein75/multi-label-emotion-classification-reddit-comments-roberta and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.bias: found shape torch.Size([28]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "- classifier.out_proj.weight: found shape torch.Size([28, 768]) in the checkpoint and torch.Size([6, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from checkpoint: ./out_multi-label-emotion-classification-reddit-comments-roberta_f1/checkpoint-9580\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10448' max='10448' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10448/10448 01:19, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.118800</td>\n",
       "      <td>1.020753</td>\n",
       "      <td>0.643643</td>\n",
       "      <td>0.763856</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 F1(macro): 0.6436\n",
      "\n",
      "[Amirhossein75/multi-label-emotion-classification-reddit-comments-roberta] Fold 2/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d7b57a8ccd049b59d69e9c1dc415092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/41787 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8590149f3a6942bfb09a56b39ab7ef08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10447 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b235214904a347eaa36615fa9933f580",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16281 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at Amirhossein75/multi-label-emotion-classification-reddit-comments-roberta and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.bias: found shape torch.Size([28]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "- classifier.out_proj.weight: found shape torch.Size([28, 768]) in the checkpoint and torch.Size([6, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from checkpoint: ./out_multi-label-emotion-classification-reddit-comments-roberta_f2/checkpoint-9580\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10448' max='10448' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10448/10448 01:22, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.101300</td>\n",
       "      <td>1.010353</td>\n",
       "      <td>0.643419</td>\n",
       "      <td>0.763473</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 F1(macro): 0.6434\n",
      "\n",
      "[Amirhossein75/multi-label-emotion-classification-reddit-comments-roberta] Fold 3/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f880862699d541b196266a1b3bd13b68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/41787 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04a8f616b6e444fa88c2ae1d965b758e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10447 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6964a143dfa6414385ecee9c7a4ba708",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16281 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at Amirhossein75/multi-label-emotion-classification-reddit-comments-roberta and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.bias: found shape torch.Size([28]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "- classifier.out_proj.weight: found shape torch.Size([28, 768]) in the checkpoint and torch.Size([6, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from checkpoint: ./out_multi-label-emotion-classification-reddit-comments-roberta_f3/checkpoint-9580\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10448' max='10448' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10448/10448 01:27, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.057100</td>\n",
       "      <td>1.038148</td>\n",
       "      <td>0.627825</td>\n",
       "      <td>0.758112</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 F1(macro): 0.6278\n",
      "\n",
      "[Amirhossein75/multi-label-emotion-classification-reddit-comments-roberta] Fold 4/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18169b5bfd5a4ae7b0ae38f16b97e6fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/41787 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9e7b20f26fb48efb68ca34914a02dac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10447 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef91bb40c5f4440fa0eeba73c605bd25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16281 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at Amirhossein75/multi-label-emotion-classification-reddit-comments-roberta and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.bias: found shape torch.Size([28]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "- classifier.out_proj.weight: found shape torch.Size([28, 768]) in the checkpoint and torch.Size([6, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from checkpoint: ./out_multi-label-emotion-classification-reddit-comments-roberta_f4/checkpoint-9580\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10448' max='10448' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10448/10448 01:22, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.121800</td>\n",
       "      <td>1.050748</td>\n",
       "      <td>0.630399</td>\n",
       "      <td>0.760697</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 F1(macro): 0.6304\n",
      "\n",
      "[Amirhossein75/multi-label-emotion-classification-reddit-comments-roberta] Fold 5/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f3699db1f8e4e959220b8c21a7bf7bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/41788 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3b99f2e283f4824934750408a4b4b01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10446 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ef3c1548d894fe08c5a48d2a1594037",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16281 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at Amirhossein75/multi-label-emotion-classification-reddit-comments-roberta and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.bias: found shape torch.Size([28]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "- classifier.out_proj.weight: found shape torch.Size([28, 768]) in the checkpoint and torch.Size([6, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from checkpoint: ./out_multi-label-emotion-classification-reddit-comments-roberta_f5/checkpoint-9580\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10448' max='10448' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10448/10448 01:18, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.090500</td>\n",
       "      <td>1.029549</td>\n",
       "      <td>0.633797</td>\n",
       "      <td>0.761631</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 F1(macro): 0.6338\n",
      "\n",
      "[Model Amirhossein75/multi-label-emotion-classification-reddit-comments-roberta] OOF F1(macro): 0.6358 | micro: 0.7616\n",
      "\n",
      "[Pseudo-Label Retrain] Model: cardiffnlp/twitter-roberta-large-emotion-latest\n",
      "Adding 3 special tokens: ['<USER>', '<URL>', '<NAME>']\n",
      "\n",
      "[cardiffnlp/twitter-roberta-large-emotion-latest] Fold 1/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75b454f56f454ff9932b803d16b0bc73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/41787 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43a794699243482b931c4184385b6280",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10447 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aed8a4b3b8bc4cabb933bcf554b63c13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16281 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-large-emotion-latest and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.bias: found shape torch.Size([11]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "- classifier.out_proj.weight: found shape torch.Size([11, 1024]) in the checkpoint and torch.Size([6, 1024]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from checkpoint: ./out_twitter-roberta-large-emotion-latest_f1/checkpoint-9580\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10448' max='10448' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10448/10448 03:05, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.033500</td>\n",
       "      <td>0.957421</td>\n",
       "      <td>0.682920</td>\n",
       "      <td>0.786063</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 F1(macro): 0.6829\n",
      "\n",
      "[cardiffnlp/twitter-roberta-large-emotion-latest] Fold 2/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8181ba1f9134a12a7739ff53a533035",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/41787 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9123433f368d44d49422935b15bd7b31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10447 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e33f26f01d784e58b22041b4f5f65515",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16281 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-large-emotion-latest and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.bias: found shape torch.Size([11]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "- classifier.out_proj.weight: found shape torch.Size([11, 1024]) in the checkpoint and torch.Size([6, 1024]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from checkpoint: ./out_twitter-roberta-large-emotion-latest_f2/checkpoint-9580\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10448' max='10448' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10448/10448 03:05, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.997000</td>\n",
       "      <td>0.953556</td>\n",
       "      <td>0.686371</td>\n",
       "      <td>0.785393</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 F1(macro): 0.6864\n",
      "\n",
      "[cardiffnlp/twitter-roberta-large-emotion-latest] Fold 3/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "654113369673474aba2f4adc3e26cef1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/41787 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "703875e175a84efea7225e1c7b67fdba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10447 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6da5266c5e4545e3a306a302d3014a35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16281 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-large-emotion-latest and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.bias: found shape torch.Size([11]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "- classifier.out_proj.weight: found shape torch.Size([11, 1024]) in the checkpoint and torch.Size([6, 1024]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from checkpoint: ./out_twitter-roberta-large-emotion-latest_f3/checkpoint-9580\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10448' max='10448' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10448/10448 03:05, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.007100</td>\n",
       "      <td>0.971006</td>\n",
       "      <td>0.668307</td>\n",
       "      <td>0.779267</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 F1(macro): 0.6683\n",
      "\n",
      "[cardiffnlp/twitter-roberta-large-emotion-latest] Fold 4/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7207ca3e95eb4019a3028e50c6fe4c4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/41787 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e94478c5ead24fc4b608bf14aabbbad0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10447 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2998f275ae944244a9f2a26aec87bb77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16281 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-large-emotion-latest and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.bias: found shape torch.Size([11]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "- classifier.out_proj.weight: found shape torch.Size([11, 1024]) in the checkpoint and torch.Size([6, 1024]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from checkpoint: ./out_twitter-roberta-large-emotion-latest_f4/checkpoint-9580\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10448' max='10448' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10448/10448 03:05, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.046900</td>\n",
       "      <td>0.981703</td>\n",
       "      <td>0.673778</td>\n",
       "      <td>0.783861</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 F1(macro): 0.6738\n",
      "\n",
      "[cardiffnlp/twitter-roberta-large-emotion-latest] Fold 5/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75449c1707064a7e9d75310d6985aed6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/41788 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f9857d0ca39481994517021ae8618bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10446 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73bd1cfb3af24e0fa12e5884adbd2271",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16281 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-large-emotion-latest and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.bias: found shape torch.Size([11]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "- classifier.out_proj.weight: found shape torch.Size([11, 1024]) in the checkpoint and torch.Size([6, 1024]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from checkpoint: ./out_twitter-roberta-large-emotion-latest_f5/checkpoint-9580\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10448' max='10448' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10448/10448 03:05, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Micro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.998900</td>\n",
       "      <td>0.964269</td>\n",
       "      <td>0.682569</td>\n",
       "      <td>0.785660</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 F1(macro): 0.6826\n",
      "\n",
      "[Model cardiffnlp/twitter-roberta-large-emotion-latest] OOF F1(macro): 0.6788 | micro: 0.7840\n",
      "Saved Full Ensemble Pseudo Submission: ./submit/submission_pseudo.csv\n"
     ]
    }
   ],
   "source": [
    "probs = test_proba\n",
    "confidences = probs.max(axis=1)\n",
    "entropy = -np.sum(probs * np.log(probs + 1e-10), axis=1)\n",
    "\n",
    "mask = (confidences > 0.95) & (entropy < 0.3)\n",
    "pseudo_df = test_df[mask].copy()\n",
    "pseudo_df[\"emotion\"] = [id2label[p] for p in probs[mask].argmax(axis=1)]\n",
    "pseudo_df[\"split\"] = \"train\" \n",
    "\n",
    "print(f\"Original Train Size: {len(train_df)}\")\n",
    "print(f\"Adding {len(pseudo_df)} pseudo-labels (Confidence > 0.95)\")\n",
    "\n",
    "train_df = pd.concat([train_df, pseudo_df], ignore_index=True)\n",
    "y = train_df[\"emotion\"].map(label2id).values.astype(int)\n",
    "\n",
    "print(f\"New Train Size: {len(train_df)}\")\n",
    "\n",
    "pseudo_all_test_logits = []\n",
    "pseudo_all_oof = []\n",
    "\n",
    "\n",
    "for name in backbones:\n",
    "    hp = cfgs.get(name, {\"lr\": 1e-5, \"epochs\": 3}) \n",
    "    \n",
    "    safe_lr = hp[\"lr\"] * 0.6 \n",
    "    \n",
    "    print(f\"\\n[Pseudo-Label Retrain] Model: {name}\")\n",
    "    \n",
    "    oof, test_logits = train_folds(\n",
    "        name,\n",
    "        n_splits=5,\n",
    "        lr=safe_lr,     \n",
    "        epochs=hp[\"epochs\"],\n",
    "        bsz=16,          \n",
    "        max_len=128\n",
    "    )\n",
    "    pseudo_all_test_logits.append(test_logits)\n",
    "    pseudo_all_oof.append(oof)\n",
    "\n",
    "final_ensemble_logits = np.mean(pseudo_all_test_logits, axis=0)\n",
    "final_preds = final_ensemble_logits.argmax(axis=1)\n",
    "final_labels = [id2label[i] for i in final_preds]\n",
    "\n",
    "sub_path = \"./submit/submission_pseudo.csv\"\n",
    "submission = pd.DataFrame({\"id\": test_df[\"id\"].values, \"emotion\": final_labels})\n",
    "submission.to_csv(sub_path, index=False)\n",
    "print(f\"Saved Full Ensemble Pseudo Submission: {sub_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Samples: 52234 | Original: 47890 | Pseudo: 4344\n",
      "\n",
      "Confusion Matrix (Rows=True, Cols=Predicted):\n",
      "[[ 7197   425   409  1157   788   718]\n",
      " [  355   443   116   105   126    38]\n",
      " [  103    46  1580   112   101    67]\n",
      " [ 1504   189   475 19453   744  1432]\n",
      " [  557    99   206   422  2433   209]\n",
      " [  539    53   140   935   283  4331]]\n",
      "\n",
      "Classification Report (Original Data Only):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.70      0.67      0.69     10694\n",
      "     disgust       0.35      0.37      0.36      1183\n",
      "        fear       0.54      0.79      0.64      2009\n",
      "         joy       0.88      0.82      0.85     23797\n",
      "     sadness       0.54      0.62      0.58      3926\n",
      "    surprise       0.64      0.69      0.66      6281\n",
      "\n",
      "    accuracy                           0.74     47890\n",
      "   macro avg       0.61      0.66      0.63     47890\n",
      "weighted avg       0.75      0.74      0.74     47890\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_pseudo = len(pseudo_df)\n",
    "n_total = len(y) \n",
    "n_original = n_total - n_pseudo\n",
    "\n",
    "print(f\"Total Samples: {n_total} | Original: {n_original} | Pseudo: {n_pseudo}\")\n",
    "\n",
    "y_real = y[:n_original]\n",
    "oof_real = pseudo_all_oof[0][:n_original] \n",
    "pred_real = oof_real.argmax(axis=1)\n",
    "\n",
    "cm = confusion_matrix(y_real, pred_real)\n",
    "print(\"\\nConfusion Matrix (Rows=True, Cols=Predicted):\")\n",
    "print(cm)\n",
    "\n",
    "print(\"\\nClassification Report (Original Data Only):\")\n",
    "print(classification_report(y_real, pred_real, target_names=labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
