{
  "best_global_step": 4790,
  "best_metric": 0.4922724295684391,
  "best_model_checkpoint": "./out_bert-base-uncased_f5/checkpoint-4790",
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 4790,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.020876826722338204,
      "grad_norm": 6.4242119789123535,
      "learning_rate": 1.4897703549060544e-05,
      "loss": 1.4367,
      "step": 50
    },
    {
      "epoch": 0.04175365344467641,
      "grad_norm": 5.244348049163818,
      "learning_rate": 1.4793319415448852e-05,
      "loss": 1.3728,
      "step": 100
    },
    {
      "epoch": 0.06263048016701461,
      "grad_norm": 15.463926315307617,
      "learning_rate": 1.468893528183716e-05,
      "loss": 1.1921,
      "step": 150
    },
    {
      "epoch": 0.08350730688935282,
      "grad_norm": 7.971535682678223,
      "learning_rate": 1.458455114822547e-05,
      "loss": 1.1783,
      "step": 200
    },
    {
      "epoch": 0.10438413361169102,
      "grad_norm": 9.374858856201172,
      "learning_rate": 1.448016701461378e-05,
      "loss": 1.1918,
      "step": 250
    },
    {
      "epoch": 0.12526096033402923,
      "grad_norm": 9.559326171875,
      "learning_rate": 1.4375782881002088e-05,
      "loss": 1.1022,
      "step": 300
    },
    {
      "epoch": 0.14613778705636743,
      "grad_norm": 5.144372463226318,
      "learning_rate": 1.4271398747390397e-05,
      "loss": 1.0467,
      "step": 350
    },
    {
      "epoch": 0.16701461377870563,
      "grad_norm": 6.777217864990234,
      "learning_rate": 1.4167014613778707e-05,
      "loss": 1.1002,
      "step": 400
    },
    {
      "epoch": 0.18789144050104384,
      "grad_norm": 6.698174476623535,
      "learning_rate": 1.4062630480167015e-05,
      "loss": 1.0919,
      "step": 450
    },
    {
      "epoch": 0.20876826722338204,
      "grad_norm": 8.538049697875977,
      "learning_rate": 1.3958246346555323e-05,
      "loss": 1.0553,
      "step": 500
    },
    {
      "epoch": 0.22964509394572025,
      "grad_norm": 5.522028923034668,
      "learning_rate": 1.3853862212943631e-05,
      "loss": 1.0368,
      "step": 550
    },
    {
      "epoch": 0.25052192066805845,
      "grad_norm": 8.710116386413574,
      "learning_rate": 1.3749478079331943e-05,
      "loss": 1.0425,
      "step": 600
    },
    {
      "epoch": 0.27139874739039666,
      "grad_norm": 6.3170552253723145,
      "learning_rate": 1.3645093945720251e-05,
      "loss": 1.0053,
      "step": 650
    },
    {
      "epoch": 0.29227557411273486,
      "grad_norm": 7.287294864654541,
      "learning_rate": 1.354070981210856e-05,
      "loss": 0.9781,
      "step": 700
    },
    {
      "epoch": 0.31315240083507306,
      "grad_norm": 7.207918167114258,
      "learning_rate": 1.343632567849687e-05,
      "loss": 0.9628,
      "step": 750
    },
    {
      "epoch": 0.33402922755741127,
      "grad_norm": 7.538695335388184,
      "learning_rate": 1.3331941544885178e-05,
      "loss": 0.9729,
      "step": 800
    },
    {
      "epoch": 0.35490605427974947,
      "grad_norm": 5.959750652313232,
      "learning_rate": 1.3227557411273486e-05,
      "loss": 1.0394,
      "step": 850
    },
    {
      "epoch": 0.3757828810020877,
      "grad_norm": 7.608830451965332,
      "learning_rate": 1.3123173277661796e-05,
      "loss": 1.0176,
      "step": 900
    },
    {
      "epoch": 0.3966597077244259,
      "grad_norm": 7.092767715454102,
      "learning_rate": 1.3018789144050106e-05,
      "loss": 0.9329,
      "step": 950
    },
    {
      "epoch": 0.4175365344467641,
      "grad_norm": 9.012402534484863,
      "learning_rate": 1.2914405010438414e-05,
      "loss": 1.0215,
      "step": 1000
    },
    {
      "epoch": 0.4384133611691023,
      "grad_norm": 7.0249223709106445,
      "learning_rate": 1.2810020876826722e-05,
      "loss": 0.969,
      "step": 1050
    },
    {
      "epoch": 0.4592901878914405,
      "grad_norm": 6.302084922790527,
      "learning_rate": 1.270563674321503e-05,
      "loss": 0.9872,
      "step": 1100
    },
    {
      "epoch": 0.4801670146137787,
      "grad_norm": 6.881286144256592,
      "learning_rate": 1.260125260960334e-05,
      "loss": 0.9565,
      "step": 1150
    },
    {
      "epoch": 0.5010438413361169,
      "grad_norm": 7.727362155914307,
      "learning_rate": 1.249686847599165e-05,
      "loss": 0.9907,
      "step": 1200
    },
    {
      "epoch": 0.5219206680584552,
      "grad_norm": 7.418371200561523,
      "learning_rate": 1.2392484342379958e-05,
      "loss": 0.9399,
      "step": 1250
    },
    {
      "epoch": 0.5427974947807933,
      "grad_norm": 8.11129379272461,
      "learning_rate": 1.2288100208768268e-05,
      "loss": 0.9826,
      "step": 1300
    },
    {
      "epoch": 0.5636743215031316,
      "grad_norm": 9.007686614990234,
      "learning_rate": 1.2183716075156576e-05,
      "loss": 0.9673,
      "step": 1350
    },
    {
      "epoch": 0.5845511482254697,
      "grad_norm": 8.197758674621582,
      "learning_rate": 1.2079331941544885e-05,
      "loss": 0.9448,
      "step": 1400
    },
    {
      "epoch": 0.605427974947808,
      "grad_norm": 9.855843544006348,
      "learning_rate": 1.1974947807933193e-05,
      "loss": 0.9372,
      "step": 1450
    },
    {
      "epoch": 0.6263048016701461,
      "grad_norm": 7.051886081695557,
      "learning_rate": 1.1870563674321505e-05,
      "loss": 0.8388,
      "step": 1500
    },
    {
      "epoch": 0.6471816283924844,
      "grad_norm": 7.856056213378906,
      "learning_rate": 1.1766179540709813e-05,
      "loss": 0.9268,
      "step": 1550
    },
    {
      "epoch": 0.6680584551148225,
      "grad_norm": 5.661942481994629,
      "learning_rate": 1.1661795407098121e-05,
      "loss": 0.9644,
      "step": 1600
    },
    {
      "epoch": 0.6889352818371608,
      "grad_norm": 7.98347806930542,
      "learning_rate": 1.1557411273486431e-05,
      "loss": 0.9809,
      "step": 1650
    },
    {
      "epoch": 0.7098121085594989,
      "grad_norm": 9.240538597106934,
      "learning_rate": 1.1453027139874739e-05,
      "loss": 0.8987,
      "step": 1700
    },
    {
      "epoch": 0.7306889352818372,
      "grad_norm": 6.476325035095215,
      "learning_rate": 1.1348643006263047e-05,
      "loss": 0.9641,
      "step": 1750
    },
    {
      "epoch": 0.7515657620041754,
      "grad_norm": 6.194445610046387,
      "learning_rate": 1.1244258872651357e-05,
      "loss": 0.9249,
      "step": 1800
    },
    {
      "epoch": 0.7724425887265136,
      "grad_norm": 6.139869689941406,
      "learning_rate": 1.1139874739039667e-05,
      "loss": 0.9404,
      "step": 1850
    },
    {
      "epoch": 0.7933194154488518,
      "grad_norm": 7.10690450668335,
      "learning_rate": 1.1035490605427975e-05,
      "loss": 0.8602,
      "step": 1900
    },
    {
      "epoch": 0.81419624217119,
      "grad_norm": 9.616143226623535,
      "learning_rate": 1.0931106471816284e-05,
      "loss": 0.9048,
      "step": 1950
    },
    {
      "epoch": 0.8350730688935282,
      "grad_norm": 5.9305572509765625,
      "learning_rate": 1.0826722338204592e-05,
      "loss": 0.9482,
      "step": 2000
    },
    {
      "epoch": 0.8559498956158664,
      "grad_norm": 7.376861572265625,
      "learning_rate": 1.0722338204592902e-05,
      "loss": 0.9516,
      "step": 2050
    },
    {
      "epoch": 0.8768267223382046,
      "grad_norm": 6.916869640350342,
      "learning_rate": 1.0617954070981212e-05,
      "loss": 0.9598,
      "step": 2100
    },
    {
      "epoch": 0.8977035490605428,
      "grad_norm": 6.939661026000977,
      "learning_rate": 1.051356993736952e-05,
      "loss": 0.9309,
      "step": 2150
    },
    {
      "epoch": 0.918580375782881,
      "grad_norm": 10.881921768188477,
      "learning_rate": 1.040918580375783e-05,
      "loss": 0.963,
      "step": 2200
    },
    {
      "epoch": 0.9394572025052192,
      "grad_norm": 8.931353569030762,
      "learning_rate": 1.0304801670146138e-05,
      "loss": 0.9129,
      "step": 2250
    },
    {
      "epoch": 0.9603340292275574,
      "grad_norm": 8.748717308044434,
      "learning_rate": 1.0200417536534446e-05,
      "loss": 0.8415,
      "step": 2300
    },
    {
      "epoch": 0.9812108559498957,
      "grad_norm": 8.475966453552246,
      "learning_rate": 1.0096033402922755e-05,
      "loss": 0.9766,
      "step": 2350
    },
    {
      "epoch": 1.0,
      "eval_f1_macro": 0.48055791128361136,
      "eval_loss": 0.9416781663894653,
      "eval_runtime": 19.1517,
      "eval_samples_per_second": 500.113,
      "eval_steps_per_second": 15.664,
      "step": 2395
    },
    {
      "epoch": 1.0020876826722338,
      "grad_norm": 6.8167595863342285,
      "learning_rate": 9.991649269311066e-06,
      "loss": 0.9663,
      "step": 2400
    },
    {
      "epoch": 1.022964509394572,
      "grad_norm": 6.419926166534424,
      "learning_rate": 9.887265135699374e-06,
      "loss": 0.8413,
      "step": 2450
    },
    {
      "epoch": 1.0438413361169103,
      "grad_norm": 7.003357887268066,
      "learning_rate": 9.782881002087683e-06,
      "loss": 0.8129,
      "step": 2500
    },
    {
      "epoch": 1.0647181628392484,
      "grad_norm": 9.941760063171387,
      "learning_rate": 9.678496868475993e-06,
      "loss": 0.7968,
      "step": 2550
    },
    {
      "epoch": 1.0855949895615866,
      "grad_norm": 9.291025161743164,
      "learning_rate": 9.5741127348643e-06,
      "loss": 0.8414,
      "step": 2600
    },
    {
      "epoch": 1.1064718162839249,
      "grad_norm": 7.969692707061768,
      "learning_rate": 9.469728601252609e-06,
      "loss": 0.8194,
      "step": 2650
    },
    {
      "epoch": 1.1273486430062631,
      "grad_norm": 7.254234790802002,
      "learning_rate": 9.365344467640919e-06,
      "loss": 0.8623,
      "step": 2700
    },
    {
      "epoch": 1.1482254697286012,
      "grad_norm": 9.98298454284668,
      "learning_rate": 9.260960334029229e-06,
      "loss": 0.8282,
      "step": 2750
    },
    {
      "epoch": 1.1691022964509394,
      "grad_norm": 6.268095970153809,
      "learning_rate": 9.156576200417537e-06,
      "loss": 0.8474,
      "step": 2800
    },
    {
      "epoch": 1.1899791231732777,
      "grad_norm": 9.176776885986328,
      "learning_rate": 9.052192066805845e-06,
      "loss": 0.8038,
      "step": 2850
    },
    {
      "epoch": 1.210855949895616,
      "grad_norm": 11.37594223022461,
      "learning_rate": 8.947807933194153e-06,
      "loss": 0.7859,
      "step": 2900
    },
    {
      "epoch": 1.231732776617954,
      "grad_norm": 9.37234115600586,
      "learning_rate": 8.843423799582463e-06,
      "loss": 0.7504,
      "step": 2950
    },
    {
      "epoch": 1.2526096033402923,
      "grad_norm": 5.841235637664795,
      "learning_rate": 8.739039665970773e-06,
      "loss": 0.7907,
      "step": 3000
    },
    {
      "epoch": 1.2734864300626305,
      "grad_norm": 6.7114973068237305,
      "learning_rate": 8.634655532359082e-06,
      "loss": 0.8276,
      "step": 3050
    },
    {
      "epoch": 1.2943632567849686,
      "grad_norm": 11.704398155212402,
      "learning_rate": 8.530271398747391e-06,
      "loss": 0.8225,
      "step": 3100
    },
    {
      "epoch": 1.3152400835073068,
      "grad_norm": 6.92057991027832,
      "learning_rate": 8.4258872651357e-06,
      "loss": 0.7296,
      "step": 3150
    },
    {
      "epoch": 1.336116910229645,
      "grad_norm": 10.658256530761719,
      "learning_rate": 8.321503131524008e-06,
      "loss": 0.8225,
      "step": 3200
    },
    {
      "epoch": 1.3569937369519833,
      "grad_norm": 8.309350967407227,
      "learning_rate": 8.217118997912316e-06,
      "loss": 0.8072,
      "step": 3250
    },
    {
      "epoch": 1.3778705636743216,
      "grad_norm": 5.621170520782471,
      "learning_rate": 8.112734864300628e-06,
      "loss": 0.7626,
      "step": 3300
    },
    {
      "epoch": 1.3987473903966596,
      "grad_norm": 8.217591285705566,
      "learning_rate": 8.008350730688936e-06,
      "loss": 0.8249,
      "step": 3350
    },
    {
      "epoch": 1.4196242171189979,
      "grad_norm": 8.67742919921875,
      "learning_rate": 7.903966597077244e-06,
      "loss": 0.8164,
      "step": 3400
    },
    {
      "epoch": 1.4405010438413361,
      "grad_norm": 8.751936912536621,
      "learning_rate": 7.799582463465552e-06,
      "loss": 0.8232,
      "step": 3450
    },
    {
      "epoch": 1.4613778705636742,
      "grad_norm": 5.534142971038818,
      "learning_rate": 7.695198329853862e-06,
      "loss": 0.8307,
      "step": 3500
    },
    {
      "epoch": 1.4822546972860124,
      "grad_norm": 9.448282241821289,
      "learning_rate": 7.590814196242171e-06,
      "loss": 0.7789,
      "step": 3550
    },
    {
      "epoch": 1.5031315240083507,
      "grad_norm": 12.632224082946777,
      "learning_rate": 7.4864300626304805e-06,
      "loss": 0.8044,
      "step": 3600
    },
    {
      "epoch": 1.524008350730689,
      "grad_norm": 11.36732292175293,
      "learning_rate": 7.38204592901879e-06,
      "loss": 0.8272,
      "step": 3650
    },
    {
      "epoch": 1.5448851774530272,
      "grad_norm": 6.982000827789307,
      "learning_rate": 7.277661795407099e-06,
      "loss": 0.768,
      "step": 3700
    },
    {
      "epoch": 1.5657620041753653,
      "grad_norm": 7.883068084716797,
      "learning_rate": 7.173277661795407e-06,
      "loss": 0.7781,
      "step": 3750
    },
    {
      "epoch": 1.5866388308977035,
      "grad_norm": 9.894984245300293,
      "learning_rate": 7.068893528183717e-06,
      "loss": 0.8233,
      "step": 3800
    },
    {
      "epoch": 1.6075156576200418,
      "grad_norm": 6.818929195404053,
      "learning_rate": 6.964509394572025e-06,
      "loss": 0.8827,
      "step": 3850
    },
    {
      "epoch": 1.6283924843423798,
      "grad_norm": 6.753486633300781,
      "learning_rate": 6.860125260960334e-06,
      "loss": 0.824,
      "step": 3900
    },
    {
      "epoch": 1.6492693110647183,
      "grad_norm": 8.752507209777832,
      "learning_rate": 6.755741127348643e-06,
      "loss": 0.8082,
      "step": 3950
    },
    {
      "epoch": 1.6701461377870563,
      "grad_norm": 9.781946182250977,
      "learning_rate": 6.651356993736952e-06,
      "loss": 0.7454,
      "step": 4000
    },
    {
      "epoch": 1.6910229645093946,
      "grad_norm": 10.345335960388184,
      "learning_rate": 6.546972860125261e-06,
      "loss": 0.793,
      "step": 4050
    },
    {
      "epoch": 1.7118997912317329,
      "grad_norm": 8.356376647949219,
      "learning_rate": 6.44258872651357e-06,
      "loss": 0.8158,
      "step": 4100
    },
    {
      "epoch": 1.732776617954071,
      "grad_norm": 10.32955265045166,
      "learning_rate": 6.3382045929018795e-06,
      "loss": 0.7594,
      "step": 4150
    },
    {
      "epoch": 1.7536534446764092,
      "grad_norm": 9.711970329284668,
      "learning_rate": 6.233820459290188e-06,
      "loss": 0.8348,
      "step": 4200
    },
    {
      "epoch": 1.7745302713987474,
      "grad_norm": 4.920783042907715,
      "learning_rate": 6.129436325678498e-06,
      "loss": 0.8691,
      "step": 4250
    },
    {
      "epoch": 1.7954070981210855,
      "grad_norm": 8.666271209716797,
      "learning_rate": 6.025052192066806e-06,
      "loss": 0.8259,
      "step": 4300
    },
    {
      "epoch": 1.816283924843424,
      "grad_norm": 8.167550086975098,
      "learning_rate": 5.920668058455115e-06,
      "loss": 0.8337,
      "step": 4350
    },
    {
      "epoch": 1.837160751565762,
      "grad_norm": 9.295929908752441,
      "learning_rate": 5.816283924843424e-06,
      "loss": 0.7778,
      "step": 4400
    },
    {
      "epoch": 1.8580375782881002,
      "grad_norm": 6.197033882141113,
      "learning_rate": 5.711899791231733e-06,
      "loss": 0.8156,
      "step": 4450
    },
    {
      "epoch": 1.8789144050104385,
      "grad_norm": 8.724212646484375,
      "learning_rate": 5.607515657620042e-06,
      "loss": 0.8243,
      "step": 4500
    },
    {
      "epoch": 1.8997912317327765,
      "grad_norm": 8.336016654968262,
      "learning_rate": 5.50313152400835e-06,
      "loss": 0.7577,
      "step": 4550
    },
    {
      "epoch": 1.9206680584551148,
      "grad_norm": 8.395316123962402,
      "learning_rate": 5.39874739039666e-06,
      "loss": 0.7905,
      "step": 4600
    },
    {
      "epoch": 1.941544885177453,
      "grad_norm": 13.224385261535645,
      "learning_rate": 5.2943632567849685e-06,
      "loss": 0.8298,
      "step": 4650
    },
    {
      "epoch": 1.962421711899791,
      "grad_norm": 8.719073295593262,
      "learning_rate": 5.1899791231732776e-06,
      "loss": 0.7662,
      "step": 4700
    },
    {
      "epoch": 1.9832985386221296,
      "grad_norm": 7.487753391265869,
      "learning_rate": 5.085594989561587e-06,
      "loss": 0.8172,
      "step": 4750
    },
    {
      "epoch": 2.0,
      "eval_f1_macro": 0.4922724295684391,
      "eval_loss": 0.9553691148757935,
      "eval_runtime": 19.2415,
      "eval_samples_per_second": 497.777,
      "eval_steps_per_second": 15.591,
      "step": 4790
    }
  ],
  "logging_steps": 50,
  "max_steps": 7185,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1309682312801280.0,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
