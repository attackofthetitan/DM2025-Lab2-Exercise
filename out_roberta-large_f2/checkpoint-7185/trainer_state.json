{
  "best_global_step": 7185,
  "best_metric": 0.5399812564836283,
  "best_model_checkpoint": "./out_roberta-large_f2/checkpoint-7185",
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 7185,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.020876826722338204,
      "grad_norm": 21.509904861450195,
      "learning_rate": 1.4897703549060544e-05,
      "loss": 1.5548,
      "step": 50
    },
    {
      "epoch": 0.04175365344467641,
      "grad_norm": 15.963869094848633,
      "learning_rate": 1.4793319415448852e-05,
      "loss": 1.4239,
      "step": 100
    },
    {
      "epoch": 0.06263048016701461,
      "grad_norm": 13.751419067382812,
      "learning_rate": 1.468893528183716e-05,
      "loss": 1.3789,
      "step": 150
    },
    {
      "epoch": 0.08350730688935282,
      "grad_norm": 45.29426956176758,
      "learning_rate": 1.458455114822547e-05,
      "loss": 1.1755,
      "step": 200
    },
    {
      "epoch": 0.10438413361169102,
      "grad_norm": 23.67610740661621,
      "learning_rate": 1.448016701461378e-05,
      "loss": 1.1986,
      "step": 250
    },
    {
      "epoch": 0.12526096033402923,
      "grad_norm": 35.949337005615234,
      "learning_rate": 1.4375782881002088e-05,
      "loss": 1.0963,
      "step": 300
    },
    {
      "epoch": 0.14613778705636743,
      "grad_norm": 23.507278442382812,
      "learning_rate": 1.4271398747390397e-05,
      "loss": 1.0472,
      "step": 350
    },
    {
      "epoch": 0.16701461377870563,
      "grad_norm": 18.39870834350586,
      "learning_rate": 1.4167014613778707e-05,
      "loss": 1.0725,
      "step": 400
    },
    {
      "epoch": 0.18789144050104384,
      "grad_norm": 19.419282913208008,
      "learning_rate": 1.4062630480167015e-05,
      "loss": 1.0566,
      "step": 450
    },
    {
      "epoch": 0.20876826722338204,
      "grad_norm": 24.265623092651367,
      "learning_rate": 1.3958246346555323e-05,
      "loss": 1.0439,
      "step": 500
    },
    {
      "epoch": 0.22964509394572025,
      "grad_norm": 23.3782901763916,
      "learning_rate": 1.3853862212943631e-05,
      "loss": 1.1427,
      "step": 550
    },
    {
      "epoch": 0.25052192066805845,
      "grad_norm": 19.33268165588379,
      "learning_rate": 1.3749478079331943e-05,
      "loss": 1.0464,
      "step": 600
    },
    {
      "epoch": 0.27139874739039666,
      "grad_norm": 19.54727554321289,
      "learning_rate": 1.3645093945720251e-05,
      "loss": 1.0469,
      "step": 650
    },
    {
      "epoch": 0.29227557411273486,
      "grad_norm": 12.202878952026367,
      "learning_rate": 1.354070981210856e-05,
      "loss": 1.1062,
      "step": 700
    },
    {
      "epoch": 0.31315240083507306,
      "grad_norm": 22.5134334564209,
      "learning_rate": 1.343632567849687e-05,
      "loss": 1.0193,
      "step": 750
    },
    {
      "epoch": 0.33402922755741127,
      "grad_norm": 17.619182586669922,
      "learning_rate": 1.3331941544885178e-05,
      "loss": 1.0088,
      "step": 800
    },
    {
      "epoch": 0.35490605427974947,
      "grad_norm": 28.983598709106445,
      "learning_rate": 1.3227557411273486e-05,
      "loss": 1.0191,
      "step": 850
    },
    {
      "epoch": 0.3757828810020877,
      "grad_norm": 11.33415412902832,
      "learning_rate": 1.3123173277661796e-05,
      "loss": 1.0296,
      "step": 900
    },
    {
      "epoch": 0.3966597077244259,
      "grad_norm": 15.881837844848633,
      "learning_rate": 1.3018789144050106e-05,
      "loss": 0.95,
      "step": 950
    },
    {
      "epoch": 0.4175365344467641,
      "grad_norm": 11.956092834472656,
      "learning_rate": 1.2914405010438414e-05,
      "loss": 0.9523,
      "step": 1000
    },
    {
      "epoch": 0.4384133611691023,
      "grad_norm": 15.574462890625,
      "learning_rate": 1.2810020876826722e-05,
      "loss": 1.0084,
      "step": 1050
    },
    {
      "epoch": 0.4592901878914405,
      "grad_norm": 15.919679641723633,
      "learning_rate": 1.270563674321503e-05,
      "loss": 1.0045,
      "step": 1100
    },
    {
      "epoch": 0.4801670146137787,
      "grad_norm": 18.56412124633789,
      "learning_rate": 1.260125260960334e-05,
      "loss": 0.9398,
      "step": 1150
    },
    {
      "epoch": 0.5010438413361169,
      "grad_norm": 13.210489273071289,
      "learning_rate": 1.249686847599165e-05,
      "loss": 0.9589,
      "step": 1200
    },
    {
      "epoch": 0.5219206680584552,
      "grad_norm": 20.77202033996582,
      "learning_rate": 1.2392484342379958e-05,
      "loss": 0.9627,
      "step": 1250
    },
    {
      "epoch": 0.5427974947807933,
      "grad_norm": 27.95306968688965,
      "learning_rate": 1.2288100208768268e-05,
      "loss": 0.9294,
      "step": 1300
    },
    {
      "epoch": 0.5636743215031316,
      "grad_norm": 18.578271865844727,
      "learning_rate": 1.2183716075156576e-05,
      "loss": 1.0016,
      "step": 1350
    },
    {
      "epoch": 0.5845511482254697,
      "grad_norm": 30.813756942749023,
      "learning_rate": 1.2079331941544885e-05,
      "loss": 0.9557,
      "step": 1400
    },
    {
      "epoch": 0.605427974947808,
      "grad_norm": 16.668045043945312,
      "learning_rate": 1.1974947807933193e-05,
      "loss": 1.0056,
      "step": 1450
    },
    {
      "epoch": 0.6263048016701461,
      "grad_norm": 9.43546199798584,
      "learning_rate": 1.1870563674321505e-05,
      "loss": 0.952,
      "step": 1500
    },
    {
      "epoch": 0.6471816283924844,
      "grad_norm": 17.187849044799805,
      "learning_rate": 1.1766179540709813e-05,
      "loss": 0.9959,
      "step": 1550
    },
    {
      "epoch": 0.6680584551148225,
      "grad_norm": 17.911569595336914,
      "learning_rate": 1.1661795407098121e-05,
      "loss": 0.9817,
      "step": 1600
    },
    {
      "epoch": 0.6889352818371608,
      "grad_norm": 20.380128860473633,
      "learning_rate": 1.1557411273486431e-05,
      "loss": 0.9235,
      "step": 1650
    },
    {
      "epoch": 0.7098121085594989,
      "grad_norm": 9.81893253326416,
      "learning_rate": 1.1453027139874739e-05,
      "loss": 1.0207,
      "step": 1700
    },
    {
      "epoch": 0.7306889352818372,
      "grad_norm": 10.27830982208252,
      "learning_rate": 1.1348643006263047e-05,
      "loss": 0.9535,
      "step": 1750
    },
    {
      "epoch": 0.7515657620041754,
      "grad_norm": 24.798484802246094,
      "learning_rate": 1.1244258872651357e-05,
      "loss": 0.937,
      "step": 1800
    },
    {
      "epoch": 0.7724425887265136,
      "grad_norm": 20.06502342224121,
      "learning_rate": 1.1139874739039667e-05,
      "loss": 0.9597,
      "step": 1850
    },
    {
      "epoch": 0.7933194154488518,
      "grad_norm": 15.236977577209473,
      "learning_rate": 1.1035490605427975e-05,
      "loss": 0.98,
      "step": 1900
    },
    {
      "epoch": 0.81419624217119,
      "grad_norm": 16.032852172851562,
      "learning_rate": 1.0931106471816284e-05,
      "loss": 0.9741,
      "step": 1950
    },
    {
      "epoch": 0.8350730688935282,
      "grad_norm": 12.580029487609863,
      "learning_rate": 1.0826722338204592e-05,
      "loss": 0.9513,
      "step": 2000
    },
    {
      "epoch": 0.8559498956158664,
      "grad_norm": 18.633411407470703,
      "learning_rate": 1.0722338204592902e-05,
      "loss": 0.9754,
      "step": 2050
    },
    {
      "epoch": 0.8768267223382046,
      "grad_norm": 13.872247695922852,
      "learning_rate": 1.0617954070981212e-05,
      "loss": 0.9279,
      "step": 2100
    },
    {
      "epoch": 0.8977035490605428,
      "grad_norm": 27.82225799560547,
      "learning_rate": 1.051356993736952e-05,
      "loss": 0.904,
      "step": 2150
    },
    {
      "epoch": 0.918580375782881,
      "grad_norm": 73.06541442871094,
      "learning_rate": 1.040918580375783e-05,
      "loss": 0.9671,
      "step": 2200
    },
    {
      "epoch": 0.9394572025052192,
      "grad_norm": 15.218446731567383,
      "learning_rate": 1.0304801670146138e-05,
      "loss": 0.8748,
      "step": 2250
    },
    {
      "epoch": 0.9603340292275574,
      "grad_norm": 8.239248275756836,
      "learning_rate": 1.0200417536534446e-05,
      "loss": 0.9996,
      "step": 2300
    },
    {
      "epoch": 0.9812108559498957,
      "grad_norm": 15.440919876098633,
      "learning_rate": 1.0096033402922755e-05,
      "loss": 0.9508,
      "step": 2350
    },
    {
      "epoch": 1.0,
      "eval_f1_macro": 0.5135793750919627,
      "eval_loss": 0.8949199914932251,
      "eval_runtime": 53.9499,
      "eval_samples_per_second": 177.535,
      "eval_steps_per_second": 5.561,
      "step": 2395
    },
    {
      "epoch": 1.0020876826722338,
      "grad_norm": 18.25052261352539,
      "learning_rate": 9.991649269311066e-06,
      "loss": 0.9364,
      "step": 2400
    },
    {
      "epoch": 1.022964509394572,
      "grad_norm": 16.09500503540039,
      "learning_rate": 9.887265135699374e-06,
      "loss": 0.8932,
      "step": 2450
    },
    {
      "epoch": 1.0438413361169103,
      "grad_norm": 17.903799057006836,
      "learning_rate": 9.782881002087683e-06,
      "loss": 0.8692,
      "step": 2500
    },
    {
      "epoch": 1.0647181628392484,
      "grad_norm": 14.117218971252441,
      "learning_rate": 9.678496868475993e-06,
      "loss": 0.8872,
      "step": 2550
    },
    {
      "epoch": 1.0855949895615866,
      "grad_norm": 20.999513626098633,
      "learning_rate": 9.5741127348643e-06,
      "loss": 0.8752,
      "step": 2600
    },
    {
      "epoch": 1.1064718162839249,
      "grad_norm": 16.175710678100586,
      "learning_rate": 9.469728601252609e-06,
      "loss": 0.8597,
      "step": 2650
    },
    {
      "epoch": 1.1273486430062631,
      "grad_norm": 16.55955696105957,
      "learning_rate": 9.365344467640919e-06,
      "loss": 0.8247,
      "step": 2700
    },
    {
      "epoch": 1.1482254697286012,
      "grad_norm": 25.986833572387695,
      "learning_rate": 9.260960334029229e-06,
      "loss": 0.8503,
      "step": 2750
    },
    {
      "epoch": 1.1691022964509394,
      "grad_norm": 19.614938735961914,
      "learning_rate": 9.156576200417537e-06,
      "loss": 0.843,
      "step": 2800
    },
    {
      "epoch": 1.1899791231732777,
      "grad_norm": 13.726936340332031,
      "learning_rate": 9.052192066805845e-06,
      "loss": 0.8685,
      "step": 2850
    },
    {
      "epoch": 1.210855949895616,
      "grad_norm": 11.571903228759766,
      "learning_rate": 8.947807933194153e-06,
      "loss": 0.8578,
      "step": 2900
    },
    {
      "epoch": 1.231732776617954,
      "grad_norm": 18.692235946655273,
      "learning_rate": 8.843423799582463e-06,
      "loss": 0.8604,
      "step": 2950
    },
    {
      "epoch": 1.2526096033402923,
      "grad_norm": 15.50664234161377,
      "learning_rate": 8.739039665970773e-06,
      "loss": 0.8276,
      "step": 3000
    },
    {
      "epoch": 1.2734864300626305,
      "grad_norm": 20.812715530395508,
      "learning_rate": 8.634655532359082e-06,
      "loss": 0.7716,
      "step": 3050
    },
    {
      "epoch": 1.2943632567849686,
      "grad_norm": 13.30947208404541,
      "learning_rate": 8.530271398747391e-06,
      "loss": 0.8434,
      "step": 3100
    },
    {
      "epoch": 1.3152400835073068,
      "grad_norm": 14.238395690917969,
      "learning_rate": 8.4258872651357e-06,
      "loss": 0.8698,
      "step": 3150
    },
    {
      "epoch": 1.336116910229645,
      "grad_norm": 19.180042266845703,
      "learning_rate": 8.321503131524008e-06,
      "loss": 0.8799,
      "step": 3200
    },
    {
      "epoch": 1.3569937369519833,
      "grad_norm": 22.226619720458984,
      "learning_rate": 8.217118997912316e-06,
      "loss": 0.86,
      "step": 3250
    },
    {
      "epoch": 1.3778705636743216,
      "grad_norm": 17.071117401123047,
      "learning_rate": 8.112734864300628e-06,
      "loss": 0.8839,
      "step": 3300
    },
    {
      "epoch": 1.3987473903966596,
      "grad_norm": 18.399934768676758,
      "learning_rate": 8.008350730688936e-06,
      "loss": 0.7835,
      "step": 3350
    },
    {
      "epoch": 1.4196242171189979,
      "grad_norm": 19.106266021728516,
      "learning_rate": 7.903966597077244e-06,
      "loss": 0.892,
      "step": 3400
    },
    {
      "epoch": 1.4405010438413361,
      "grad_norm": 22.032466888427734,
      "learning_rate": 7.799582463465552e-06,
      "loss": 0.88,
      "step": 3450
    },
    {
      "epoch": 1.4613778705636742,
      "grad_norm": 17.83550453186035,
      "learning_rate": 7.695198329853862e-06,
      "loss": 0.821,
      "step": 3500
    },
    {
      "epoch": 1.4822546972860124,
      "grad_norm": 17.2919921875,
      "learning_rate": 7.590814196242171e-06,
      "loss": 0.8939,
      "step": 3550
    },
    {
      "epoch": 1.5031315240083507,
      "grad_norm": 34.60234832763672,
      "learning_rate": 7.4864300626304805e-06,
      "loss": 0.8614,
      "step": 3600
    },
    {
      "epoch": 1.524008350730689,
      "grad_norm": 13.743734359741211,
      "learning_rate": 7.38204592901879e-06,
      "loss": 0.9063,
      "step": 3650
    },
    {
      "epoch": 1.5448851774530272,
      "grad_norm": 14.644251823425293,
      "learning_rate": 7.277661795407099e-06,
      "loss": 0.8239,
      "step": 3700
    },
    {
      "epoch": 1.5657620041753653,
      "grad_norm": 13.341946601867676,
      "learning_rate": 7.173277661795407e-06,
      "loss": 0.793,
      "step": 3750
    },
    {
      "epoch": 1.5866388308977035,
      "grad_norm": 6.505046844482422,
      "learning_rate": 7.068893528183717e-06,
      "loss": 0.8912,
      "step": 3800
    },
    {
      "epoch": 1.6075156576200418,
      "grad_norm": 16.412065505981445,
      "learning_rate": 6.964509394572025e-06,
      "loss": 0.895,
      "step": 3850
    },
    {
      "epoch": 1.6283924843423798,
      "grad_norm": 18.26413917541504,
      "learning_rate": 6.860125260960334e-06,
      "loss": 0.8285,
      "step": 3900
    },
    {
      "epoch": 1.6492693110647183,
      "grad_norm": 8.235616683959961,
      "learning_rate": 6.755741127348643e-06,
      "loss": 0.8442,
      "step": 3950
    },
    {
      "epoch": 1.6701461377870563,
      "grad_norm": 18.830202102661133,
      "learning_rate": 6.651356993736952e-06,
      "loss": 0.8632,
      "step": 4000
    },
    {
      "epoch": 1.6910229645093946,
      "grad_norm": 16.212657928466797,
      "learning_rate": 6.546972860125261e-06,
      "loss": 0.8343,
      "step": 4050
    },
    {
      "epoch": 1.7118997912317329,
      "grad_norm": 17.27412986755371,
      "learning_rate": 6.44258872651357e-06,
      "loss": 0.78,
      "step": 4100
    },
    {
      "epoch": 1.732776617954071,
      "grad_norm": 16.629283905029297,
      "learning_rate": 6.3382045929018795e-06,
      "loss": 0.8355,
      "step": 4150
    },
    {
      "epoch": 1.7536534446764092,
      "grad_norm": 17.39645767211914,
      "learning_rate": 6.233820459290188e-06,
      "loss": 0.8689,
      "step": 4200
    },
    {
      "epoch": 1.7745302713987474,
      "grad_norm": 18.441484451293945,
      "learning_rate": 6.129436325678498e-06,
      "loss": 0.833,
      "step": 4250
    },
    {
      "epoch": 1.7954070981210855,
      "grad_norm": 14.964444160461426,
      "learning_rate": 6.025052192066806e-06,
      "loss": 0.7961,
      "step": 4300
    },
    {
      "epoch": 1.816283924843424,
      "grad_norm": 18.821571350097656,
      "learning_rate": 5.920668058455115e-06,
      "loss": 0.8252,
      "step": 4350
    },
    {
      "epoch": 1.837160751565762,
      "grad_norm": 17.985782623291016,
      "learning_rate": 5.816283924843424e-06,
      "loss": 0.8853,
      "step": 4400
    },
    {
      "epoch": 1.8580375782881002,
      "grad_norm": 21.047157287597656,
      "learning_rate": 5.711899791231733e-06,
      "loss": 0.7906,
      "step": 4450
    },
    {
      "epoch": 1.8789144050104385,
      "grad_norm": 15.446428298950195,
      "learning_rate": 5.607515657620042e-06,
      "loss": 0.8727,
      "step": 4500
    },
    {
      "epoch": 1.8997912317327765,
      "grad_norm": 15.053168296813965,
      "learning_rate": 5.50313152400835e-06,
      "loss": 0.8019,
      "step": 4550
    },
    {
      "epoch": 1.9206680584551148,
      "grad_norm": 13.724078178405762,
      "learning_rate": 5.39874739039666e-06,
      "loss": 0.8319,
      "step": 4600
    },
    {
      "epoch": 1.941544885177453,
      "grad_norm": 20.29422378540039,
      "learning_rate": 5.2943632567849685e-06,
      "loss": 0.8828,
      "step": 4650
    },
    {
      "epoch": 1.962421711899791,
      "grad_norm": 21.5019588470459,
      "learning_rate": 5.1899791231732776e-06,
      "loss": 0.8374,
      "step": 4700
    },
    {
      "epoch": 1.9832985386221296,
      "grad_norm": 18.603666305541992,
      "learning_rate": 5.085594989561587e-06,
      "loss": 0.8875,
      "step": 4750
    },
    {
      "epoch": 2.0,
      "eval_f1_macro": 0.5317596799937828,
      "eval_loss": 0.8819834589958191,
      "eval_runtime": 54.1006,
      "eval_samples_per_second": 177.04,
      "eval_steps_per_second": 5.545,
      "step": 4790
    },
    {
      "epoch": 2.0041753653444676,
      "grad_norm": 14.177000999450684,
      "learning_rate": 4.981210855949896e-06,
      "loss": 0.8335,
      "step": 4800
    },
    {
      "epoch": 2.0250521920668056,
      "grad_norm": 16.700048446655273,
      "learning_rate": 4.876826722338204e-06,
      "loss": 0.7203,
      "step": 4850
    },
    {
      "epoch": 2.045929018789144,
      "grad_norm": 21.120746612548828,
      "learning_rate": 4.772442588726514e-06,
      "loss": 0.7906,
      "step": 4900
    },
    {
      "epoch": 2.066805845511482,
      "grad_norm": 17.459644317626953,
      "learning_rate": 4.668058455114823e-06,
      "loss": 0.6988,
      "step": 4950
    },
    {
      "epoch": 2.0876826722338206,
      "grad_norm": 29.065858840942383,
      "learning_rate": 4.563674321503131e-06,
      "loss": 0.7093,
      "step": 5000
    },
    {
      "epoch": 2.1085594989561587,
      "grad_norm": 18.850730895996094,
      "learning_rate": 4.459290187891441e-06,
      "loss": 0.7356,
      "step": 5050
    },
    {
      "epoch": 2.1294363256784967,
      "grad_norm": 23.643592834472656,
      "learning_rate": 4.354906054279749e-06,
      "loss": 0.7406,
      "step": 5100
    },
    {
      "epoch": 2.150313152400835,
      "grad_norm": 15.48291301727295,
      "learning_rate": 4.250521920668058e-06,
      "loss": 0.7861,
      "step": 5150
    },
    {
      "epoch": 2.1711899791231732,
      "grad_norm": 28.031463623046875,
      "learning_rate": 4.1461377870563675e-06,
      "loss": 0.6842,
      "step": 5200
    },
    {
      "epoch": 2.1920668058455113,
      "grad_norm": 13.448346138000488,
      "learning_rate": 4.0417536534446765e-06,
      "loss": 0.6815,
      "step": 5250
    },
    {
      "epoch": 2.2129436325678498,
      "grad_norm": 9.51729965209961,
      "learning_rate": 3.937369519832985e-06,
      "loss": 0.6842,
      "step": 5300
    },
    {
      "epoch": 2.233820459290188,
      "grad_norm": 27.104028701782227,
      "learning_rate": 3.832985386221295e-06,
      "loss": 0.7903,
      "step": 5350
    },
    {
      "epoch": 2.2546972860125263,
      "grad_norm": 28.087127685546875,
      "learning_rate": 3.7286012526096033e-06,
      "loss": 0.7411,
      "step": 5400
    },
    {
      "epoch": 2.2755741127348643,
      "grad_norm": 21.371166229248047,
      "learning_rate": 3.6242171189979124e-06,
      "loss": 0.7241,
      "step": 5450
    },
    {
      "epoch": 2.2964509394572024,
      "grad_norm": 16.330509185791016,
      "learning_rate": 3.5198329853862215e-06,
      "loss": 0.7176,
      "step": 5500
    },
    {
      "epoch": 2.317327766179541,
      "grad_norm": 19.009536743164062,
      "learning_rate": 3.41544885177453e-06,
      "loss": 0.7854,
      "step": 5550
    },
    {
      "epoch": 2.338204592901879,
      "grad_norm": 18.049482345581055,
      "learning_rate": 3.311064718162839e-06,
      "loss": 0.701,
      "step": 5600
    },
    {
      "epoch": 2.359081419624217,
      "grad_norm": 25.74359703063965,
      "learning_rate": 3.2066805845511487e-06,
      "loss": 0.7382,
      "step": 5650
    },
    {
      "epoch": 2.3799582463465554,
      "grad_norm": 28.290821075439453,
      "learning_rate": 3.1022964509394573e-06,
      "loss": 0.7338,
      "step": 5700
    },
    {
      "epoch": 2.4008350730688934,
      "grad_norm": 19.984100341796875,
      "learning_rate": 2.9979123173277664e-06,
      "loss": 0.7396,
      "step": 5750
    },
    {
      "epoch": 2.421711899791232,
      "grad_norm": 20.257020950317383,
      "learning_rate": 2.8935281837160755e-06,
      "loss": 0.6831,
      "step": 5800
    },
    {
      "epoch": 2.44258872651357,
      "grad_norm": 14.674079895019531,
      "learning_rate": 2.789144050104384e-06,
      "loss": 0.7828,
      "step": 5850
    },
    {
      "epoch": 2.463465553235908,
      "grad_norm": 28.923450469970703,
      "learning_rate": 2.684759916492693e-06,
      "loss": 0.7409,
      "step": 5900
    },
    {
      "epoch": 2.4843423799582465,
      "grad_norm": 17.300949096679688,
      "learning_rate": 2.5803757828810023e-06,
      "loss": 0.7669,
      "step": 5950
    },
    {
      "epoch": 2.5052192066805845,
      "grad_norm": 14.666824340820312,
      "learning_rate": 2.475991649269311e-06,
      "loss": 0.7286,
      "step": 6000
    },
    {
      "epoch": 2.526096033402923,
      "grad_norm": 15.640847206115723,
      "learning_rate": 2.37160751565762e-06,
      "loss": 0.7118,
      "step": 6050
    },
    {
      "epoch": 2.546972860125261,
      "grad_norm": 16.976001739501953,
      "learning_rate": 2.267223382045929e-06,
      "loss": 0.739,
      "step": 6100
    },
    {
      "epoch": 2.567849686847599,
      "grad_norm": 20.90107536315918,
      "learning_rate": 2.162839248434238e-06,
      "loss": 0.6685,
      "step": 6150
    },
    {
      "epoch": 2.588726513569937,
      "grad_norm": 33.95936584472656,
      "learning_rate": 2.0584551148225472e-06,
      "loss": 0.7069,
      "step": 6200
    },
    {
      "epoch": 2.6096033402922756,
      "grad_norm": 44.34841537475586,
      "learning_rate": 1.9540709812108563e-06,
      "loss": 0.7033,
      "step": 6250
    },
    {
      "epoch": 2.6304801670146136,
      "grad_norm": 20.049358367919922,
      "learning_rate": 1.849686847599165e-06,
      "loss": 0.6987,
      "step": 6300
    },
    {
      "epoch": 2.651356993736952,
      "grad_norm": 22.795724868774414,
      "learning_rate": 1.745302713987474e-06,
      "loss": 0.6981,
      "step": 6350
    },
    {
      "epoch": 2.67223382045929,
      "grad_norm": 28.088088989257812,
      "learning_rate": 1.6409185803757829e-06,
      "loss": 0.6999,
      "step": 6400
    },
    {
      "epoch": 2.693110647181628,
      "grad_norm": 22.833269119262695,
      "learning_rate": 1.5365344467640917e-06,
      "loss": 0.7838,
      "step": 6450
    },
    {
      "epoch": 2.7139874739039667,
      "grad_norm": 14.28383731842041,
      "learning_rate": 1.432150313152401e-06,
      "loss": 0.735,
      "step": 6500
    },
    {
      "epoch": 2.7348643006263047,
      "grad_norm": 23.749006271362305,
      "learning_rate": 1.3277661795407099e-06,
      "loss": 0.6993,
      "step": 6550
    },
    {
      "epoch": 2.755741127348643,
      "grad_norm": 31.21175193786621,
      "learning_rate": 1.2233820459290187e-06,
      "loss": 0.7105,
      "step": 6600
    },
    {
      "epoch": 2.776617954070981,
      "grad_norm": 23.849613189697266,
      "learning_rate": 1.1189979123173278e-06,
      "loss": 0.7304,
      "step": 6650
    },
    {
      "epoch": 2.7974947807933193,
      "grad_norm": 16.473438262939453,
      "learning_rate": 1.0146137787056367e-06,
      "loss": 0.7309,
      "step": 6700
    },
    {
      "epoch": 2.8183716075156577,
      "grad_norm": 22.599546432495117,
      "learning_rate": 9.102296450939457e-07,
      "loss": 0.7199,
      "step": 6750
    },
    {
      "epoch": 2.8392484342379958,
      "grad_norm": 26.463598251342773,
      "learning_rate": 8.058455114822547e-07,
      "loss": 0.6864,
      "step": 6800
    },
    {
      "epoch": 2.8601252609603343,
      "grad_norm": 19.495405197143555,
      "learning_rate": 7.014613778705637e-07,
      "loss": 0.7173,
      "step": 6850
    },
    {
      "epoch": 2.8810020876826723,
      "grad_norm": 21.44808578491211,
      "learning_rate": 5.970772442588728e-07,
      "loss": 0.6907,
      "step": 6900
    },
    {
      "epoch": 2.9018789144050103,
      "grad_norm": 23.637332916259766,
      "learning_rate": 4.926931106471816e-07,
      "loss": 0.7136,
      "step": 6950
    },
    {
      "epoch": 2.9227557411273484,
      "grad_norm": 23.283485412597656,
      "learning_rate": 3.8830897703549064e-07,
      "loss": 0.6556,
      "step": 7000
    },
    {
      "epoch": 2.943632567849687,
      "grad_norm": 15.224442481994629,
      "learning_rate": 2.839248434237996e-07,
      "loss": 0.6962,
      "step": 7050
    },
    {
      "epoch": 2.964509394572025,
      "grad_norm": 16.0566349029541,
      "learning_rate": 1.7954070981210854e-07,
      "loss": 0.6693,
      "step": 7100
    },
    {
      "epoch": 2.9853862212943634,
      "grad_norm": 22.158082962036133,
      "learning_rate": 7.515657620041754e-08,
      "loss": 0.6851,
      "step": 7150
    },
    {
      "epoch": 3.0,
      "eval_f1_macro": 0.5399812564836283,
      "eval_loss": 0.9320343732833862,
      "eval_runtime": 54.2622,
      "eval_samples_per_second": 176.513,
      "eval_steps_per_second": 5.529,
      "step": 7185
    }
  ],
  "logging_steps": 50,
  "max_steps": 7185,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 7234506210560640.0,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
